{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f554972-a84a-441c-8565-589f338a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import ee\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openet.core\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import ipyplot\n",
    "\n",
    "# gsutil -m rm \"gs://openet_temp/skip_scene_stats/2025/*.csv\"\n",
    "# gsutil -m cp \"gs://openet_temp/skip_scene_stats/2025/*.csv\" ./stats/2025/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f809b90-fa61-43b6-b3c3-cb365ec6ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(\n",
    "    project='ee-cmorton',\n",
    "    opt_url='https://earthengine-highvolume.googleapis.com'\n",
    ")\n",
    "\n",
    "stats_ws = os.path.join(os.getcwd(), 'stats')\n",
    "if not os.path.isdir(stats_ws):\n",
    "    os.makedirs(stats_ws)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f175af-c860-4ddf-b9d1-72224e0f7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrs2_skip_list = [\n",
    "    'p010r030', \n",
    "]\n",
    "\n",
    "wrs2_list = sorted(\n",
    "    ee.FeatureCollection('projects/openet/assets/features/wrs2/custom')\n",
    "    .filterBounds(ee.Geometry.BBox(-124, 26, -67.9, 50))\n",
    "    .filter(ee.Filter.inList('wrs2_tile', wrs2_skip_list).Not())\n",
    "    .aggregate_histogram('wrs2_tile').keys().getInfo(),\n",
    "    reverse=True\n",
    ")\n",
    "# print(len(wrs2_list))\n",
    "\n",
    "ocean_wrs2_list = [\n",
    "    'p048r027', 'p047r031', 'p047r030', 'p047r029', 'p046r033', \n",
    "    'p045r034', 'p044r035', 'p043r036', 'p041r037', 'p040r038', \n",
    "    'p038r041', 'p038r040',\n",
    "    'p025r040', 'p024r040', 'p024r027', 'p023r040', \n",
    "    'p023r027', 'p022r040', 'p021r040', 'p020r029',\n",
    "    'p017r041', 'p016r038', 'p015r040', 'p015r037', \n",
    "    'p013r033', 'p012r032', 'p011r031', 'p011r030', \n",
    "]\n",
    "\n",
    "california_wrs2_list = [\n",
    "    'p038r036', 'p038r037', \n",
    "    'p039r035', 'p039r036', 'p039r037',\n",
    "    'p040r034', 'p040r035', 'p040r036', 'p040r037',\n",
    "    'p041r034', 'p041r035', 'p041r036', 'p041r037',\n",
    "    'p042r033', 'p042r034', 'p042r035', 'p042r036',\n",
    "    'p043r031', 'p043r032', 'p043r033', 'p043r034', 'p043r035',\n",
    "    'p044r031', 'p044r032', 'p044r033', 'p044r034',\n",
    "    'p045r031', 'p045r032', 'p045r033',\n",
    "    'p046r031', 'p046r032', 'p047r031',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58b512-59c5-473d-8cd5-447317382df8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "land_mask = ee.Image('projects/openet/assets/features/water_mask').Not()\n",
    "# Apply the NLCD/NALCMS water mask (anywhere it is water, set the ocean mask \n",
    "land_mask = land_mask.where(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).eq(18), 0)\n",
    "# land_mask = land_mask.And(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).neq(18))\n",
    "# # land_mask = ee.Image('projects/openet/assets/meteorology/conus404/ancillary/land_mask')\n",
    "\n",
    "# etf_coll_id = 'projects/openet/assets/ssebop/conus/gridmet/landsat/c02'\n",
    "etf_coll_id = 'projects/usgs-gee-nhm-ssebop/assets/ssebop/landsat/c02'\n",
    "# etf_coll_id = 'projects/openet/assets/intercomparison/ssebop/landsat/c02/v0p2p6'\n",
    "band_name = 'et_fraction'\n",
    "\n",
    "rgb_bands = {\n",
    "    'LT04': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LT05': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LE07': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LC08': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "    'LC09': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "}\n",
    "\n",
    "# 0 - white, 1 - no fill (green), 2 - shadow (dark blue), 3 - snow (light blue), 4 - cloud (light gray), 5 - water (purple), 6 - ocean mask\n",
    "fmask_palette = \"ffffff, 9effa1, blue, 00aff2, dddddd, purple, bfbfbf\"\n",
    "fmask_max = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91566e57-a06c-4da0-8c28-8e91aa2db0ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Intercomparison sites and dates\n",
    "\n",
    "# sites_csv = '/Users/Charles.Morton@dri.edu/Projects/openet-tools/intercomparison/master_flux_station_list.csv'\n",
    "# sites_df = pd.read_csv(sites_csv)\n",
    "\n",
    "# interp_days = 32\n",
    "# site_keep_list = []\n",
    "# wrs2_delimiter = ';'\n",
    "\n",
    "# # Hardcoding the sites CSV field names for now\n",
    "# start_field = 'START_DATE'\n",
    "# end_field = 'END_DATE'\n",
    "# site_field = 'SITE_ID'\n",
    "# lat_field = 'LATITUDE'\n",
    "# lon_field = 'LONGITUDE'\n",
    "# wrs2_field = 'WRS2_TILES'\n",
    "\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# # Group the date ranges by WRS2 tile\n",
    "# # print(f'\\nGrouping overlapping dates')\n",
    "# wrs2_dates = collections.defaultdict(list)\n",
    "# wrs2_sites = collections.defaultdict(list)\n",
    "# for (site_i, site) in sites_df.iterrows():\n",
    "#     # print(site_i)\n",
    "#     # print(site)\n",
    "#     if site['RANDOM_SELECTION'] not in [0, 1]:\n",
    "#         # print('  Unsupported RANDOM_SELECTION value')\n",
    "#         input('ENTER')\n",
    "#         continue\n",
    "#     # if site['RANDOM_SELECTION'] != 1:\n",
    "#     #     continue\n",
    "#     if site_keep_list and site.loc[site_field] not in site_keep_list:\n",
    "#         # print('  Site not in keep list - skipping')\n",
    "#         continue\n",
    "\n",
    "#     # Include all sites in INI file, even those outside the date range\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_sites[wrs2.strip()].append([\n",
    "#             round(site.loc[lon_field], 6), round(site.loc[lat_field], 6)\n",
    "#         ])\n",
    "\n",
    "#     start_dt = datetime.datetime.strptime(site.loc[start_field], '%Y-%m-%d')\n",
    "#     end_dt = datetime.datetime.strptime(site.loc[end_field], '%Y-%m-%d')\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # If start/end dates are within N gap days of the start/end of the month\n",
    "#     #   consider it a \"full\" month\n",
    "#     gap_days = 5\n",
    "#     # print('  Snapping start date to month')\n",
    "#     month_start_dt = datetime.datetime(start_dt.year, start_dt.month, 1)\n",
    "#     if (start_dt - month_start_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         start_dt = month_start_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         start_dt = month_start_dt + relativedelta(months=1)\n",
    "\n",
    "#     # print('  Snapping end date to month')\n",
    "#     month_end_dt = end_dt + relativedelta(months=1)\n",
    "#     month_end_dt = datetime.datetime(month_end_dt.year, month_end_dt.month, 1)\n",
    "#     month_end_dt = month_end_dt - relativedelta(days=1)\n",
    "#     if (month_end_dt - end_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         end_dt = month_end_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         end_dt = (month_end_dt + relativedelta(days=1) -\n",
    "#                   relativedelta(months=1) - relativedelta(days=1))\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     if interp_days > 0:\n",
    "#         # Buffer the date ranges by the interpolate days value if set\n",
    "#         # print('  Buffering start/end dates')\n",
    "#         start_dt = start_dt - datetime.timedelta(days=interp_days)\n",
    "#         end_dt = end_dt + datetime.timedelta(days=interp_days)\n",
    "#         # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # CM - Changing conditionals to get single date ranges to work\n",
    "#     # if end_dt <= start_dt or start_dt >= end_dt:\n",
    "#     if end_dt < start_dt or start_dt > end_dt:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print('  Date range outside min/max, skipping')\n",
    "#         continue\n",
    "#     else:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         pass\n",
    "\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_dates[wrs2.strip()].append([start_dt, end_dt])\n",
    "\n",
    "# # pprint.pprint(wrs2_dates)\n",
    "\n",
    "# # Merge the date ranges that overlap\n",
    "# print(f'\\nMerging overlapping dates')\n",
    "# merged_dates = {}\n",
    "# for wrs2, dates in sorted(wrs2_dates.items()):\n",
    "#     # print(f'  {wrs2}')\n",
    "#     # pprint.pprint(sorted(dates))\n",
    "\n",
    "#     # Push the first interval on to the stack\n",
    "#     merged_dates[wrs2] = [sorted(dates)[0]]\n",
    "\n",
    "#     # Only check for overlapping ranges if there is more than 1 range\n",
    "#     if len(dates) == 1:\n",
    "#         continue\n",
    "\n",
    "#     for d in sorted(dates)[1:]:\n",
    "#         # If the current date range doesn't overlap, add it to the stack\n",
    "#         if d[0] > merged_dates[wrs2][-1][1]:\n",
    "#             merged_dates[wrs2].append(d)\n",
    "#         # If the ranges overlap and the end date is later,\n",
    "#         #   update the end time of the stack value\n",
    "#         elif ((d[0] <= merged_dates[wrs2][-1][1]) and\n",
    "#               (d[1] > merged_dates[wrs2][-1][1])):\n",
    "#             merged_dates[wrs2][-1][1] = d[1]\n",
    "\n",
    "# # pprint.pprint(merged_dates)\n",
    "\n",
    "# # # CGM - Splitting by year for DisALEXI is not needed if the NLCD\n",
    "# # #   is set to the image collection instead of the image\n",
    "# # # For DisALEXI split the date ranges by year after merging\n",
    "# # # For other models, index by the first year in the range\n",
    "# # # This may be functionality we will want for other models later\n",
    "# # year_dates = collections.defaultdict(dict)\n",
    "# # # if model in ['DISALEXI_TAIR_10K', 'DISALEXI_TAIR_1K', 'DISALEXI', 'DISALEXI_TAIR_DIRECT']:\n",
    "# # #     for wrs2, dates in merged_dates.items():\n",
    "# # #         # split_dates = {}\n",
    "# # #         for date_i, date in enumerate(dates):\n",
    "# # #             for year in range(date[0].year, date[1].year+1):\n",
    "# # #                 year_date = [\n",
    "# # #                     max(date[0], datetime.datetime(year, 1, 1)),\n",
    "# # #                     min(date[1], datetime.datetime(year, 12, 31)),\n",
    "# # #                 ]\n",
    "# # #                 try:\n",
    "# # #                     year_dates[wrs2][year].append(year_date)\n",
    "# # #                 except:\n",
    "# # #                     year_dates[wrs2][year] = [year_date]\n",
    "# # # else:\n",
    "# # for wrs2, dates in merged_dates.items():\n",
    "# #     year_dates[wrs2][dates[0][0].year] = dates\n",
    "\n",
    "# # pprint.pprint(year_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdaea1-e7ee-4ed3-a392-4f44f02bd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmask(landsat_img):\n",
    "    # Add the fmask image on top of the true color image\n",
    "    qa_img = landsat_img.select('QA_PIXEL')\n",
    "    fill_mask = qa_img.bitwiseAnd(1).neq(0)                  # bits: 0\n",
    "    dilate_mask = qa_img.rightShift(1).bitwiseAnd(1).neq(0)  # bits: 1\n",
    "    cirrus_mask = qa_img.rightShift(2).bitwiseAnd(1).neq(0)  # bits: 2\n",
    "    cloud_mask = qa_img.rightShift(3).bitwiseAnd(1).neq(0)   # bits: 3\n",
    "    shadow_mask = qa_img.rightShift(4).bitwiseAnd(1).neq(0)  # bits: 4\n",
    "    snow_mask = qa_img.rightShift(5).bitwiseAnd(1).neq(0)    # bits: 5\n",
    "    clear_mask = qa_img.rightShift(6).bitwiseAnd(1).neq(0)   # bits: 6\n",
    "    water_mask = qa_img.rightShift(7).bitwiseAnd(1).neq(0)   # bits: 7\n",
    "    cloud_conf = qa_img.rightShift(8).bitwiseAnd(3)          # bits: 8, 9\n",
    "    shadow_conf = qa_img.rightShift(10).bitwiseAnd(3)        # bits: 10, 11\n",
    "    snow_conf = qa_img.rightShift(12).bitwiseAnd(3)          # bits: 12, 13\n",
    "    cirrus_conf = qa_img.rightShift(14).bitwiseAnd(3)        # bits: 14, 15\n",
    "\n",
    "    # Saturated pixels\n",
    "    # Flag as saturated if any of the RGB bands are saturated\n",
    "    #   or change .gt(0) to .gt(7) to flag if all RGB bands are saturated\n",
    "    # Comment out rightShift line to flag if saturated in any band\n",
    "    bitshift = ee.Dictionary({'LANDSAT_4': 0, 'LANDSAT_5': 0, 'LANDSAT_7': 0, 'LANDSAT_8': 1, 'LANDSAT_9': 1});\n",
    "    saturated_mask = (\n",
    "        landsat_img.select('QA_RADSAT')\n",
    "        .rightShift(ee.Number(bitshift.get(ee.String(landsat_img.get('SPACECRAFT_ID'))))).bitwiseAnd(7)\n",
    "        .gt(0)\n",
    "    )\n",
    "    \n",
    "    # Old \"Fmask\" style image\n",
    "    fmask_img = (\n",
    "        qa_img.multiply(0)\n",
    "        .where(landsat_img.select(['SR_B4']).mask().eq(0), 1)\n",
    "        # .where(saturated_mask, 6)\n",
    "        .where(water_mask, 5)\n",
    "        .where(shadow_mask, 2)\n",
    "        .where(snow_mask, 3)\n",
    "        .where(cloud_mask.Or(dilate_mask).Or(cirrus_mask), 4)\n",
    "        # .add(shadow_mask.multiply(2))\n",
    "        # .add(snow_mask.multiply(3))\n",
    "        # .add(cloud_mask.Or(dilate_mask).Or(cirrus_mask).multiply(4))\n",
    "        # .add(cloud_mask.Or(dilate_mask).multiply(4))\n",
    "        # .add(cloud_mask.And(cloud_conf).multiply(4))\n",
    "        # .add(water_mask.multiply(5))\n",
    "    )\n",
    "    \n",
    "    return fmask_img.updateMask(fmask_img.neq(0)).rename(['fmask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784957e0-dec8-41ca-a833-9776631cfa2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the scene skip list file\n",
    "skip_path = '../v2p1.csv'\n",
    "#skip_path = '../v2p1_eemetric_error.csv'\n",
    "print(f'\\n{skip_path}')\n",
    "\n",
    "with open(skip_path, 'r') as csv_f:\n",
    "    scene_skip_lines = csv_f.readlines()\n",
    "scene_skip_header = scene_skip_lines.pop(0)\n",
    "\n",
    "# Drop the comments and empty lines\n",
    "scene_skip_lines = [line.strip() for line in scene_skip_lines if line.strip() and line[0] != '#']\n",
    "\n",
    "# Sort by date then by tile\n",
    "scene_skip_lines = sorted(scene_skip_lines, key=lambda x:x.split(',')[0].split('_')[-1] + '_' + x.split(',')[0].split('_')[-2])\n",
    "\n",
    "# Identify duplicate scene IDs (as opposed to duplicate lines)\n",
    "# Note, this block is not removing any lines, just printing\n",
    "print('Duplicate Scene IDs:')\n",
    "\n",
    "if len({l.split(',')[0] for l in scene_skip_lines}) != len(scene_skip_lines):\n",
    "    for item, count in collections.Counter([l.split(',')[0] for l in scene_skip_lines]).items():\n",
    "        if count > 1:\n",
    "            print(item)\n",
    "\n",
    "# Identify lines with no reason\n",
    "print('\\nMissing reason Scene IDs:')\n",
    "for l in scene_skip_lines:\n",
    "    if ',' not in l:\n",
    "        print(l)\n",
    "    elif l.split(',')[1].strip() == '':\n",
    "        print(l)\n",
    "    elif len(l.split(',')) > 2:\n",
    "        print(l)\n",
    "\n",
    "# # Identify duplicate lines (not duplicate SCENE IDs)\n",
    "# if len({line for line in scene_skip_lines}) != len(scene_skip_lines):\n",
    "#     print('Duplicate Lines:')\n",
    "#     for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#         if count > 1:\n",
    "#             print(item)\n",
    "# \n",
    "#     # # Uncomment to have the tool remove duplicate lines\n",
    "#     # scene_remove_lines = []\n",
    "#     # for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#     #     if count > 1:\n",
    "#     #         scene_remove_lines.append(item)\n",
    "#     #         # print(item)\n",
    "#      \n",
    "#     # # Does this only remove the first one?\n",
    "#     # if scene_remove_lines:\n",
    "#     #     print(f'Removing {len(scene_remove_lines)} duplicate lines in file')\n",
    "#     #     for line in scene_remove_lines:\n",
    "#     #         print(line)\n",
    "#     #         scene_skip_lines.remove(line)\n",
    "# \n",
    "# # Then recheck for duplicate SCENE_IDs (but different notes or dates)\n",
    "# scenes = {line.split(',')[0] for line in scene_skip_lines}           \n",
    "# if len(scenes) != len(scene_skip_lines):\n",
    "#     print('Duplicate scene IDs still in file')\n",
    "    \n",
    "print('\\nWriting updated scene skip list CSV')\n",
    "with open(skip_path.replace('.csv', '_sorted.csv'), 'w') as csv_f:\n",
    "    csv_f.write(scene_skip_header)\n",
    "    for i, line in enumerate(scene_skip_lines):\n",
    "        csv_f.write(line + '\\n')\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb643-0c14-4dec-90e5-e392adfd47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the EEMETRIC skip list by merging the EEMETRIC error list and the full skip list\n",
    "scene_skip_path = '../v2p1_sorted.csv'\n",
    "eemetric_error_path = '../v2p1_eemetric_error.csv'\n",
    "eemetric_skip_path = '../v2p1_eemetric.csv'\n",
    "\n",
    "with open(eemetric_error_path, 'r') as csv_f:\n",
    "    eemetric_skip_lines = csv_f.readlines()\n",
    "    \n",
    "with open(scene_skip_path, 'r') as csv_f:\n",
    "    scene_skip_lines = csv_f.readlines()\n",
    "\n",
    "print('\\nWriting eemetric scene skip list CSV')\n",
    "with open(eemetric_skip_path, 'w') as csv_f:\n",
    "    for i, line in enumerate(eemetric_skip_lines):\n",
    "        csv_f.write(line)\n",
    "    csv_f.write('\\n')\n",
    "    for i, line in enumerate(scene_skip_lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        csv_f.write(line)\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7135e03-10dd-4577-9b6d-11b1383f5feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61faa3e-a250-486c-b34f-cb4b57fe6971",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Print scenes with high masked count percentages\n",
    "# count_threshold_pct_min = 75\n",
    "# count_threshold_pct_max = 80\n",
    "count_threshold_pct_min = 80\n",
    "count_threshold_pct_max = 101\n",
    "\n",
    "#start_year = 1984\n",
    "#start_year = 2003\n",
    "#start_year = 2015\n",
    "#start_year = 2022\n",
    "start_year = 2025\n",
    "end_year = 2025\n",
    "years = list(range(start_year, end_year + 1))\n",
    "#months = [6, 7, 8]\n",
    "months = []\n",
    "\n",
    "print_count = 10\n",
    "image_size = 1400\n",
    "#image_size = 700\n",
    "\n",
    "# Read in the scene skip list\n",
    "scene_skip_url = '../v2p1.csv'\n",
    "# scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "print('Reading image stats CSV files')\n",
    "stats_df_list = []\n",
    "for wrs2_tile in wrs2_list:\n",
    "    # if int(wrs2_tile[1:4]) not in range(10, 25):\n",
    "    #     continue\n",
    "        \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "        if not os.path.isfile(wrs2_stats_path):\n",
    "            # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "            continue\n",
    "        try:\n",
    "            wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "        except Exception as e:\n",
    "            print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "            continue\n",
    "        if wrs2_stats_df.empty:\n",
    "            continue\n",
    "        wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "        wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "        stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# Skip the Landsat 7 scenes in 2023\n",
    "l7_2022_mask = (\n",
    "    (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "    (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    ")\n",
    "stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# Only check specific months scenes\n",
    "if months:\n",
    "    stats_df = stats_df[stats_df['DATE'].str.slice(4,6).astype(int).isin(months)]\n",
    "\n",
    "# Compute the ratios\n",
    "# stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SHADOW_COUNT_RATIO'] = stats_df['SHADOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['MASKED_PIXELS'] = (\n",
    "    stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "    + stats_df['SHADOW_PIXELS']\n",
    "    + stats_df['SNOW_PIXELS']\n",
    "    + stats_df['WATER_PIXELS']\n",
    "    + stats_df['ACCA_PIXELS']\n",
    "    # + stats_df['SATURATED_PIXELS']\n",
    ")\n",
    "stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# Work through the tiles based on which ones already have the most skipped scenes\n",
    "wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# wrs2_tiles = ['']\n",
    "\n",
    "new_skip_scenes = []\n",
    "new_skip_count = 0\n",
    "\n",
    "wrs2_i = 0\n",
    "\n",
    "# for wrs2 in reversed(wrs2_tiles):\n",
    "# for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "    if wrs2_i >= 20:\n",
    "        break\n",
    "    if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "        continue\n",
    "    # if california_wrs2_list and (wrs2 not in california_wrs2_list) and wrs2 not in ['p042r033']:\n",
    "    #     continue\n",
    "    # if int(wrs2[1:4]) != 10:\n",
    "    #     continue\n",
    "    # if int(wrs2[1:4]) != 24:\n",
    "    #     continue\n",
    "    # if int(wrs2[5:8]) >= 30:\n",
    "    #     continue\n",
    "    #if int(wrs2[5:8]) == 25 or int(wrs2[5:8]) == 50:\n",
    "    #   continue\n",
    "    print(wrs2)\n",
    "    \n",
    "    wrs2_path = int(wrs2[1:4])\n",
    "    wrs2_row = int(wrs2[5:8])\n",
    "    wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "    wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "    wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "    wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "    # Applying skip list here so that main stats DF has all scenes\n",
    "    wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "    wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "    \n",
    "    # Filter on the overall cloud count ratio\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "    wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "    # # Filter on the CLOUD_COVER_LAND property\n",
    "    # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "    # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 69]\n",
    "    #wrs2_stats_df.sort_values('CLOUD_COVER_LAND', ascending=False, inplace=True)\n",
    "\n",
    "    if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "        continue\n",
    "    print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "    wrs2_skip_scenes = []\n",
    "    wrs2_skip_count = 0\n",
    "    \n",
    "    # for i, row in wrs2_stats_df.iterrows():\n",
    "    for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "\n",
    "        scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "        above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "        above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "        if len(above_stats_df):\n",
    "            above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            above_cloud_pct = None\n",
    "            \n",
    "        below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "        below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "        if len(below_stats_df):\n",
    "            below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            below_cloud_pct = None\n",
    "\n",
    "        # # Only show scenes that have above & below both skipped or None\n",
    "        # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "        #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "        #     continue   \n",
    "\n",
    "        # # Only show scenes that have either above & below skipped or None\n",
    "        # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) and \n",
    "        #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "        #     continue   \n",
    "            \n",
    "        landsat_type = scene_id.split('_')[0].upper()\n",
    "        landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "        landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "        # Landsat true color image\n",
    "        landsat_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "            .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        # Landsat true color with Fmask\n",
    "        fmask_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "            .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "            .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        print('#'*80)\n",
    "        print(\n",
    "            f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "            f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "            f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "        )\n",
    "        ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "        # Show the images above and below the target wrs2\n",
    "        above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "        above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            above_url = (\n",
    "                above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            above_url = None\n",
    "            \n",
    "        below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "        below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            below_url = (\n",
    "                below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            below_url = None\n",
    "        \n",
    "        above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "        below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "        if above_url and below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "        elif above_url:\n",
    "            print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([above_url], img_width=image_size)\n",
    "        elif below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "            ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "        wrs2_skip_scenes.append(scene_id)\n",
    "        wrs2_skip_count += 1\n",
    "        if wrs2_skip_count >= print_count:\n",
    "            break\n",
    "\n",
    "    if wrs2_skip_scenes:\n",
    "        wrs2_i += 1\n",
    "        for scene_id in wrs2_skip_scenes:\n",
    "            print(scene_id)\n",
    "        new_skip_scenes.extend(wrs2_skip_scenes)\n",
    "\n",
    "print('\\nNew Skip Scenes')\n",
    "if new_skip_scenes:\n",
    "    for scene_id in new_skip_scenes:\n",
    "        print(scene_id)\n",
    "\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f528d7d-456d-40c5-ae04-da87629da0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
