{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f554972-a84a-441c-8565-589f338a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import ee\n",
    "import ipyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openet.core\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# %matplotlib nbagg\n",
    "# %matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import jscatter\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Output, VBox\n",
    "\n",
    "import pyperclip\n",
    "\n",
    "#from IPython.display import Image, display\n",
    "#from ipywidgets import widgets, interactive\n",
    "\n",
    "# gsutil -m rm \"gs://openet_temp/skip_scene_stats/2025/*.csv\"\n",
    "# gsutil -m cp \"gs://openet_temp/skip_scene_stats/2025/*.csv\" ./stats/2025/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f809b90-fa61-43b6-b3c3-cb365ec6ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(\n",
    "    project='ee-cmorton',\n",
    "    opt_url='https://earthengine-highvolume.googleapis.com'\n",
    ")\n",
    "\n",
    "stats_ws = os.path.join(os.getcwd(), 'stats')\n",
    "if not os.path.isdir(stats_ws):\n",
    "    os.makedirs(stats_ws)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f175af-c860-4ddf-b9d1-72224e0f7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark WRS2 list\n",
    "wrs2_list = [\n",
    "    'p198r021', 'p198r020', \n",
    "    'p197r022', 'p197r021', 'p197r020', \n",
    "    'p196r022', 'p196r021', 'p196r020', \n",
    "    'p195r022', 'p195r021', \n",
    "    'p194r021', 'p194r022'\n",
    "]\n",
    "\n",
    "wrs2_skip_list = [\n",
    "    # 'p010r030', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58b512-59c5-473d-8cd5-447317382df8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "land_mask = ee.Image('projects/openet/assets/features/water_mask').Not()\n",
    "# Apply the NLCD/NALCMS water mask (anywhere it is water, set the ocean mask \n",
    "land_mask = land_mask.where(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).eq(18), 0)\n",
    "# land_mask = land_mask.And(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).neq(18))\n",
    "# # land_mask = ee.Image('projects/openet/assets/meteorology/conus404/ancillary/land_mask')\n",
    "\n",
    "# etf_coll_id = 'projects/openet/assets/ssebop/conus/gridmet/landsat/c02'\n",
    "# etf_coll_id = 'projects/usgs-gee-nhm-ssebop/assets/ssebop/landsat/c02'\n",
    "# etf_coll_id = 'projects/openet/assets/intercomparison/ssebop/landsat/c02/v0p2p6'\n",
    "band_name = 'et_fraction'\n",
    "\n",
    "rgb_bands = {\n",
    "    'LT04': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LT05': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LE07': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LC08': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "    'LC09': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "}\n",
    "\n",
    "# 0 - white, 1 - no fill (green), 2 - shadow (dark blue), 3 - snow (light blue), 4 - cloud (light gray), 5 - water (purple), 6 - ocean mask\n",
    "fmask_palette = \"ffffff, 9effa1, blue, 00aff2, dddddd, purple, bfbfbf\"\n",
    "fmask_max = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91566e57-a06c-4da0-8c28-8e91aa2db0ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Intercomparison sites and dates\n",
    "\n",
    "# sites_csv = '/Users/Charles.Morton@dri.edu/Projects/openet-tools/intercomparison/master_flux_station_list.csv'\n",
    "# sites_df = pd.read_csv(sites_csv)\n",
    "\n",
    "# interp_days = 32\n",
    "# site_keep_list = []\n",
    "# wrs2_delimiter = ';'\n",
    "\n",
    "# # Hardcoding the sites CSV field names for now\n",
    "# start_field = 'START_DATE'\n",
    "# end_field = 'END_DATE'\n",
    "# site_field = 'SITE_ID'\n",
    "# lat_field = 'LATITUDE'\n",
    "# lon_field = 'LONGITUDE'\n",
    "# wrs2_field = 'WRS2_TILES'\n",
    "\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# # Group the date ranges by WRS2 tile\n",
    "# # print(f'\\nGrouping overlapping dates')\n",
    "# wrs2_dates = collections.defaultdict(list)\n",
    "# wrs2_sites = collections.defaultdict(list)\n",
    "# for (site_i, site) in sites_df.iterrows():\n",
    "#     # print(site_i)\n",
    "#     # print(site)\n",
    "#     if site['RANDOM_SELECTION'] not in [0, 1]:\n",
    "#         # print('  Unsupported RANDOM_SELECTION value')\n",
    "#         input('ENTER')\n",
    "#         continue\n",
    "#     # if site['RANDOM_SELECTION'] != 1:\n",
    "#     #     continue\n",
    "#     if site_keep_list and site.loc[site_field] not in site_keep_list:\n",
    "#         # print('  Site not in keep list - skipping')\n",
    "#         continue\n",
    "\n",
    "#     # Include all sites in INI file, even those outside the date range\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_sites[wrs2.strip()].append([\n",
    "#             round(site.loc[lon_field], 6), round(site.loc[lat_field], 6)\n",
    "#         ])\n",
    "\n",
    "#     start_dt = datetime.datetime.strptime(site.loc[start_field], '%Y-%m-%d')\n",
    "#     end_dt = datetime.datetime.strptime(site.loc[end_field], '%Y-%m-%d')\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # If start/end dates are within N gap days of the start/end of the month\n",
    "#     #   consider it a \"full\" month\n",
    "#     gap_days = 5\n",
    "#     # print('  Snapping start date to month')\n",
    "#     month_start_dt = datetime.datetime(start_dt.year, start_dt.month, 1)\n",
    "#     if (start_dt - month_start_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         start_dt = month_start_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         start_dt = month_start_dt + relativedelta(months=1)\n",
    "\n",
    "#     # print('  Snapping end date to month')\n",
    "#     month_end_dt = end_dt + relativedelta(months=1)\n",
    "#     month_end_dt = datetime.datetime(month_end_dt.year, month_end_dt.month, 1)\n",
    "#     month_end_dt = month_end_dt - relativedelta(days=1)\n",
    "#     if (month_end_dt - end_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         end_dt = month_end_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         end_dt = (month_end_dt + relativedelta(days=1) -\n",
    "#                   relativedelta(months=1) - relativedelta(days=1))\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     if interp_days > 0:\n",
    "#         # Buffer the date ranges by the interpolate days value if set\n",
    "#         # print('  Buffering start/end dates')\n",
    "#         start_dt = start_dt - datetime.timedelta(days=interp_days)\n",
    "#         end_dt = end_dt + datetime.timedelta(days=interp_days)\n",
    "#         # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # CM - Changing conditionals to get single date ranges to work\n",
    "#     # if end_dt <= start_dt or start_dt >= end_dt:\n",
    "#     if end_dt < start_dt or start_dt > end_dt:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print('  Date range outside min/max, skipping')\n",
    "#         continue\n",
    "#     else:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         pass\n",
    "\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_dates[wrs2.strip()].append([start_dt, end_dt])\n",
    "\n",
    "# # pprint.pprint(wrs2_dates)\n",
    "\n",
    "# # Merge the date ranges that overlap\n",
    "# print(f'\\nMerging overlapping dates')\n",
    "# merged_dates = {}\n",
    "# for wrs2, dates in sorted(wrs2_dates.items()):\n",
    "#     # print(f'  {wrs2}')\n",
    "#     # pprint.pprint(sorted(dates))\n",
    "\n",
    "#     # Push the first interval on to the stack\n",
    "#     merged_dates[wrs2] = [sorted(dates)[0]]\n",
    "\n",
    "#     # Only check for overlapping ranges if there is more than 1 range\n",
    "#     if len(dates) == 1:\n",
    "#         continue\n",
    "\n",
    "#     for d in sorted(dates)[1:]:\n",
    "#         # If the current date range doesn't overlap, add it to the stack\n",
    "#         if d[0] > merged_dates[wrs2][-1][1]:\n",
    "#             merged_dates[wrs2].append(d)\n",
    "#         # If the ranges overlap and the end date is later,\n",
    "#         #   update the end time of the stack value\n",
    "#         elif ((d[0] <= merged_dates[wrs2][-1][1]) and\n",
    "#               (d[1] > merged_dates[wrs2][-1][1])):\n",
    "#             merged_dates[wrs2][-1][1] = d[1]\n",
    "\n",
    "# # pprint.pprint(merged_dates)\n",
    "\n",
    "# # # CGM - Splitting by year for DisALEXI is not needed if the NLCD\n",
    "# # #   is set to the image collection instead of the image\n",
    "# # # For DisALEXI split the date ranges by year after merging\n",
    "# # # For other models, index by the first year in the range\n",
    "# # # This may be functionality we will want for other models later\n",
    "# # year_dates = collections.defaultdict(dict)\n",
    "# # # if model in ['DISALEXI_TAIR_10K', 'DISALEXI_TAIR_1K', 'DISALEXI', 'DISALEXI_TAIR_DIRECT']:\n",
    "# # #     for wrs2, dates in merged_dates.items():\n",
    "# # #         # split_dates = {}\n",
    "# # #         for date_i, date in enumerate(dates):\n",
    "# # #             for year in range(date[0].year, date[1].year+1):\n",
    "# # #                 year_date = [\n",
    "# # #                     max(date[0], datetime.datetime(year, 1, 1)),\n",
    "# # #                     min(date[1], datetime.datetime(year, 12, 31)),\n",
    "# # #                 ]\n",
    "# # #                 try:\n",
    "# # #                     year_dates[wrs2][year].append(year_date)\n",
    "# # #                 except:\n",
    "# # #                     year_dates[wrs2][year] = [year_date]\n",
    "# # # else:\n",
    "# # for wrs2, dates in merged_dates.items():\n",
    "# #     year_dates[wrs2][dates[0][0].year] = dates\n",
    "\n",
    "# # pprint.pprint(year_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdaea1-e7ee-4ed3-a392-4f44f02bd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmask(landsat_img):\n",
    "    # Add the fmask image on top of the true color image\n",
    "    qa_img = landsat_img.select('QA_PIXEL')\n",
    "    fill_mask = qa_img.bitwiseAnd(1).neq(0)                  # bits: 0\n",
    "    dilate_mask = qa_img.rightShift(1).bitwiseAnd(1).neq(0)  # bits: 1\n",
    "    cirrus_mask = qa_img.rightShift(2).bitwiseAnd(1).neq(0)  # bits: 2\n",
    "    cloud_mask = qa_img.rightShift(3).bitwiseAnd(1).neq(0)   # bits: 3\n",
    "    shadow_mask = qa_img.rightShift(4).bitwiseAnd(1).neq(0)  # bits: 4\n",
    "    snow_mask = qa_img.rightShift(5).bitwiseAnd(1).neq(0)    # bits: 5\n",
    "    clear_mask = qa_img.rightShift(6).bitwiseAnd(1).neq(0)   # bits: 6\n",
    "    water_mask = qa_img.rightShift(7).bitwiseAnd(1).neq(0)   # bits: 7\n",
    "    cloud_conf = qa_img.rightShift(8).bitwiseAnd(3)          # bits: 8, 9\n",
    "    shadow_conf = qa_img.rightShift(10).bitwiseAnd(3)        # bits: 10, 11\n",
    "    snow_conf = qa_img.rightShift(12).bitwiseAnd(3)          # bits: 12, 13\n",
    "    cirrus_conf = qa_img.rightShift(14).bitwiseAnd(3)        # bits: 14, 15\n",
    "\n",
    "    # Saturated pixels\n",
    "    # Flag as saturated if any of the RGB bands are saturated\n",
    "    #   or change .gt(0) to .gt(7) to flag if all RGB bands are saturated\n",
    "    # Comment out rightShift line to flag if saturated in any band\n",
    "    bitshift = ee.Dictionary({'LANDSAT_4': 0, 'LANDSAT_5': 0, 'LANDSAT_7': 0, 'LANDSAT_8': 1, 'LANDSAT_9': 1});\n",
    "    saturated_mask = (\n",
    "        landsat_img.select('QA_RADSAT')\n",
    "        .rightShift(ee.Number(bitshift.get(ee.String(landsat_img.get('SPACECRAFT_ID'))))).bitwiseAnd(7)\n",
    "        .gt(0)\n",
    "    )\n",
    "    \n",
    "    # Old \"Fmask\" style image\n",
    "    fmask_img = (\n",
    "        qa_img.multiply(0)\n",
    "        .where(landsat_img.select(['SR_B4']).mask().eq(0), 1)\n",
    "        # .where(saturated_mask, 6)\n",
    "        .where(water_mask, 5)\n",
    "        .where(shadow_mask, 2)\n",
    "        .where(snow_mask, 3)\n",
    "        .where(cloud_mask.Or(dilate_mask).Or(cirrus_mask), 4)\n",
    "        # .add(shadow_mask.multiply(2))\n",
    "        # .add(snow_mask.multiply(3))\n",
    "        # .add(cloud_mask.Or(dilate_mask).Or(cirrus_mask).multiply(4))\n",
    "        # .add(cloud_mask.Or(dilate_mask).multiply(4))\n",
    "        # .add(cloud_mask.And(cloud_conf).multiply(4))\n",
    "        # .add(water_mask.multiply(5))\n",
    "    )\n",
    "    \n",
    "    return fmask_img.updateMask(fmask_img.neq(0)).rename(['fmask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784957e0-dec8-41ca-a833-9776631cfa2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the scene skip list file\n",
    "skip_path = '../v2p1_denmark.csv'\n",
    "print(f'\\n{skip_path}')\n",
    "\n",
    "with open(skip_path, 'r') as csv_f:\n",
    "    scene_skip_lines = csv_f.readlines()\n",
    "scene_skip_header = scene_skip_lines.pop(0)\n",
    "\n",
    "# Drop the comments and empty lines\n",
    "scene_skip_lines = [line.strip() for line in scene_skip_lines if line.strip() and line[0] != '#']\n",
    "\n",
    "# Sort by date then by tile\n",
    "scene_skip_lines = sorted(scene_skip_lines, key=lambda x:x.split(',')[0].split('_')[-1] + '_' + x.split(',')[0].split('_')[-2])\n",
    "\n",
    "# Identify duplicate scene IDs (as opposed to duplicate lines)\n",
    "# Note, this block is not removing any lines, just printing\n",
    "print('Duplicate Scene IDs:')\n",
    "\n",
    "if len({l.split(',')[0] for l in scene_skip_lines}) != len(scene_skip_lines):\n",
    "    for item, count in collections.Counter([l.split(',')[0] for l in scene_skip_lines]).items():\n",
    "        if count > 1:\n",
    "            print(item)\n",
    "\n",
    "# Identify lines with no reason\n",
    "print('\\nMissing reason Scene IDs:')\n",
    "for l in scene_skip_lines:\n",
    "    if ',' not in l:\n",
    "        print(l)\n",
    "    elif l.split(',')[1].strip() == '':\n",
    "        print(l)\n",
    "    elif len(l.split(',')) > 2:\n",
    "        print(l)\n",
    "\n",
    "# # Identify duplicate lines (not duplicate SCENE IDs)\n",
    "# if len({line for line in scene_skip_lines}) != len(scene_skip_lines):\n",
    "#     print('Duplicate Lines:')\n",
    "#     for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#         if count > 1:\n",
    "#             print(item)\n",
    "# \n",
    "#     # # Uncomment to have the tool remove duplicate lines\n",
    "#     # scene_remove_lines = []\n",
    "#     # for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#     #     if count > 1:\n",
    "#     #         scene_remove_lines.append(item)\n",
    "#     #         # print(item)\n",
    "#      \n",
    "#     # # Does this only remove the first one?\n",
    "#     # if scene_remove_lines:\n",
    "#     #     print(f'Removing {len(scene_remove_lines)} duplicate lines in file')\n",
    "#     #     for line in scene_remove_lines:\n",
    "#     #         print(line)\n",
    "#     #         scene_skip_lines.remove(line)\n",
    "# \n",
    "# # Then recheck for duplicate SCENE_IDs (but different notes or dates)\n",
    "# scenes = {line.split(',')[0] for line in scene_skip_lines}           \n",
    "# if len(scenes) != len(scene_skip_lines):\n",
    "#     print('Duplicate scene IDs still in file')\n",
    "    \n",
    "print('\\nWriting updated scene skip list CSV')\n",
    "with open(skip_path.replace('.csv', '_sorted.csv'), 'w') as csv_f:\n",
    "    csv_f.write(scene_skip_header)\n",
    "    for i, line in enumerate(scene_skip_lines):\n",
    "        csv_f.write(line + '\\n')\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb643-0c14-4dec-90e5-e392adfd47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the EEMETRIC skip list by merging the EEMETRIC error list and the full skip list\n",
    "# scene_skip_path = '../v2p1_sorted.csv'\n",
    "# eemetric_error_path = '../v2p1_eemetric_error.csv'\n",
    "# eemetric_skip_path = '../v2p1_eemetric.csv'\n",
    "\n",
    "# with open(eemetric_error_path, 'r') as csv_f:\n",
    "#     eemetric_skip_lines = csv_f.readlines()\n",
    "    \n",
    "# with open(scene_skip_path, 'r') as csv_f:\n",
    "#     scene_skip_lines = csv_f.readlines()\n",
    "\n",
    "# print('\\nWriting eemetric scene skip list CSV')\n",
    "# with open(eemetric_skip_path, 'w') as csv_f:\n",
    "#     for i, line in enumerate(eemetric_skip_lines):\n",
    "#         csv_f.write(line)\n",
    "#     csv_f.write('\\n')\n",
    "#     for i, line in enumerate(scene_skip_lines):\n",
    "#         if i == 0:\n",
    "#             continue\n",
    "#         csv_f.write(line)\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7135e03-10dd-4577-9b6d-11b1383f5feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61faa3e-a250-486c-b34f-cb4b57fe6971",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the full CSV archive\n",
    "start_year = 2017\n",
    "end_year = 2026\n",
    "years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# Read in the scene skip list\n",
    "scene_skip_url = '../v2p1_denmark.csv'\n",
    "# scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "# # Add the cloudscore images to the skip list\n",
    "# scene_skip_list = sorted(list(set(scene_skip_list + scene_cloudscore_list)))\n",
    "\n",
    "\n",
    "print('Reading image stats CSV files')\n",
    "stats_df_list = []\n",
    "for wrs2_i, wrs2_tile in enumerate(wrs2_list):\n",
    "    if wrs2_i % 100 == 0:\n",
    "        print(f'  {wrs2_i:>3d}')\n",
    "        \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "        if not os.path.isfile(wrs2_stats_path):\n",
    "            # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "            continue\n",
    "        try:\n",
    "            wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "        except Exception as e:\n",
    "            print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "            continue\n",
    "        if wrs2_stats_df.empty:\n",
    "            continue\n",
    "\n",
    "        # # Only keep images that have a cloud cover land below the default threshold\n",
    "        # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "\n",
    "        # Add separate fields for the scene ID components\n",
    "        wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 16) + '-' + wrs2_stats_df['SCENE_ID'].str.slice(16, 18) + '-' + wrs2_stats_df['SCENE_ID'].str.slice(18, 20)\n",
    "        wrs2_stats_df['YEAR'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 16).astype(int)\n",
    "        wrs2_stats_df['MONTH'] = wrs2_stats_df['SCENE_ID'].str.slice(16, 18).astype(int)\n",
    "        wrs2_stats_df['LANDSAT'] = wrs2_stats_df['SCENE_ID'].str.slice(0, 4)\n",
    "        wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "        wrs2_stats_df.drop(['.geo', 'system:index'], axis=1, inplace=True)\n",
    "\n",
    "        # Skip the most recent images since the MORAN stats aren't being computed\n",
    "        # wrs2_stats_df = wrs2_stats_df[~((wrs2_stats_df['YEAR'] >= 2025) & (wrs2_stats_df['MONTH'] >= 6))]\n",
    "\n",
    "        # # Remove all of the images with a negative moran value\n",
    "        # # This will help catch images that are 100% cloud (because of shadow)\n",
    "        # #   but will also remove any image that we haven't computed MORAN stats for\n",
    "        # wrs2_stats_df[wrs2_stats_df['MORAN_1K'] <= -0.1]\n",
    "\n",
    "        # # Overwrite the negative moran and reflectance values\n",
    "        # # These seem to mainly happen when the image is 100% masked (because of shadow)\n",
    "        # #   and could probably be combined into a single call vased on the pixel count\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['MORAN_1K'] < 0, 'MORAN_1K'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['MORAN_2K'] < 0, 'MORAN_2K'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['MORAN_4K'] < 0, 'MORAN_4K'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['MORAN_8K'] < 0, 'MORAN_8K'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_SR_RED'] < -0.1, 'UNMASKED_SR_RED'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_SR_GREEN'] < -0.1, 'UNMASKED_SR_GREEN'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_SR_BLUE'] < -0.1, 'UNMASKED_SR_BLUE'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_TOA_RED'] < -0.1, 'UNMASKED_TOA_RED'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_TOA_GREEN'] < -0.1, 'UNMASKED_TOA_GREEN'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['UNMASKED_TOA_BLUE'] < -0.1, 'UNMASKED_TOA_BLUE'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['SR_RED'] < -0.1, 'SR_RED'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['SR_GREEN'] < -0.1, 'SR_GREEN'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['SR_BLUE'] < -0.1, 'SR_BLUE'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['TOA_RED'] < -0.1, 'TOA_RED'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['TOA_GREEN'] < -0.1, 'TOA_GREEN'] = 1\n",
    "        # wrs2_stats_df.loc[wrs2_stats_df['TOA_BLUE'] < -0.1, 'TOA_BLUE'] = 1\n",
    "\n",
    "        # Compute the average reflectance values for the RGB bands\n",
    "        #   and then remove the separate RGB bands\n",
    "        wrs2_stats_df['UNMASKED_SR'] = (wrs2_stats_df['UNMASKED_SR_RED'] + wrs2_stats_df['UNMASKED_SR_GREEN'] + wrs2_stats_df['UNMASKED_SR_BLUE']) / 3\n",
    "        wrs2_stats_df['UNMASKED_TOA'] = (wrs2_stats_df['UNMASKED_TOA_RED'] + wrs2_stats_df['UNMASKED_TOA_GREEN'] + wrs2_stats_df['UNMASKED_TOA_BLUE']) / 3\n",
    "        wrs2_stats_df['MASKED_SR'] = (wrs2_stats_df['SR_RED'] + wrs2_stats_df['SR_GREEN'] + wrs2_stats_df['SR_BLUE']) / 3\n",
    "        wrs2_stats_df['MASKED_TOA'] = (wrs2_stats_df['TOA_RED'] + wrs2_stats_df['TOA_GREEN'] + wrs2_stats_df['TOA_BLUE']) / 3\n",
    "        wrs2_stats_df.drop(['UNMASKED_SR_RED', 'UNMASKED_SR_GREEN', 'UNMASKED_SR_BLUE'], axis=1, inplace=True)\n",
    "        wrs2_stats_df.drop(['UNMASKED_TOA_RED', 'UNMASKED_TOA_GREEN', 'UNMASKED_TOA_BLUE'], axis=1, inplace=True)\n",
    "        # wrs2_stats_df.drop(['SR_RED', 'SR_GREEN', 'SR_BLUE'], axis=1, inplace=True)\n",
    "        wrs2_stats_df.drop(['TOA_RED', 'TOA_GREEN', 'TOA_BLUE'], axis=1, inplace=True)\n",
    "\n",
    "        if not wrs2_stats_df.empty:\n",
    "            stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# Skip the Landsat 7 scenes in 2023\n",
    "l7_2022_mask = (stats_df['DATE'].str.slice(0,4) >= '2022') & (stats_df['LANDSAT'] == 'LE07')\n",
    "stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# Compute the ratios\n",
    "stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['SHADOW_COUNT_RATIO'] = stats_df['SHADOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['SATURATED_COUNT_RATIO'] = stats_df['SATURATED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['MASKED_PIXELS'] = (\n",
    "    stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "    + stats_df['SHADOW_PIXELS']\n",
    "    + stats_df['SNOW_PIXELS']\n",
    "    + stats_df['WATER_PIXELS']\n",
    "    + stats_df['ACCA_PIXELS']\n",
    "    # + stats_df['SATURATED_PIXELS']\n",
    ")\n",
    "stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# Only keep images that are less than 100% cloudy based on the composite cloud masked computed above\n",
    "# This should catch the images that have the shadow mask set over all land pixels\n",
    "# stats_df = stats_df[stats_df['CLOUD_COUNT_RATIO'] < 1]\n",
    "\n",
    "# Set the skip reason in this order so that the more rare ones are written last\n",
    "stats_df['SKIPPED'] = 'None'\n",
    "skip_ids = {\n",
    "    'Cloud': scene_skip_df[scene_skip_df['REASON'].str.contains('Cloud')]['SCENE_ID'].values,\n",
    "    'Snow': scene_skip_df[scene_skip_df['REASON'].str.contains('Snow')]['SCENE_ID'].values,\n",
    "    'Cirrus': scene_skip_df[scene_skip_df['REASON'].str.contains('Cirrus')]['SCENE_ID'].values,\n",
    "    'Smoke': scene_skip_df[scene_skip_df['REASON'].str.contains('Smoke')]['SCENE_ID'].values,\n",
    "    'Shadow': scene_skip_df[scene_skip_df['REASON'].str.contains('Shadow')]['SCENE_ID'].values,\n",
    "    'Missing': scene_skip_df[scene_skip_df['REASON'].str.contains('Missing')]['SCENE_ID'].values,\n",
    "    # 'Cloudscore': scene_cloudscore_list,\n",
    "    # 'Weird': \n",
    "    # 'Bad': \n",
    "}\n",
    "for key, values in skip_ids.items():\n",
    "    stats_df.loc[stats_df['SCENE_ID'].isin(skip_ids[key]), 'SKIPPED'] = key\n",
    "\n",
    "print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# Work through the tiles based on which ones already have the most skipped scenes\n",
    "wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0da3d3-95e4-4e03-83f4-fb335b4ff4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33d1e6-306f-4b07-9316-1ba2628c8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "reasons = ['None', 'Snow', 'Cirrus', 'Shadow', 'Error', 'Smoke', 'Missing', 'Cloudscore']\n",
    "\n",
    "def plot_timeseries(data_df, x, y, reasons, height=800, width=1600):\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(height=height, width=width, hovermode='closest')\n",
    "\n",
    "    if y in ['UNMASKED_SR', 'UNMASKED_TOA']:\n",
    "        fig.update_layout(yaxis_range=[-0.01, 0.31])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_df[x], y=data_df[y], text=data_df['SCENE_ID'], \n",
    "        name='Full', mode='markers', showlegend=False,\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color='rgba(0, 0, 0, 0)',\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    for reason in reasons:\n",
    "        if len(wrs2_stats_df.loc[data_df['SKIPPED']==reason, y]) == 0:\n",
    "            continue\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data_df.loc[data_df['SKIPPED']==reason, x], \n",
    "            y=data_df.loc[data_df['SKIPPED']==reason, y], \n",
    "            text=data_df.loc[data_df['SKIPPED']==reason, 'SCENE_ID'],\n",
    "            name=reason, mode='markers', hoverinfo='skip',\n",
    "            marker=dict(\n",
    "                symbol='circle' if reason == 'None' else 'triangle-up',\n",
    "                size=8,\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    out = Output()\n",
    "    @out.capture(clear_output=True)\n",
    "    def do_click(trace, points, selector):\n",
    "        if points.point_inds:\n",
    "            pyperclip.copy(data_df.SCENE_ID.iloc[points.point_inds[0]])\n",
    "            print(f'{data_df.SCENE_ID.iloc[points.point_inds[0]]}')\n",
    "    fig.data[0].on_click(do_click)\n",
    "    return fig, out\n",
    "    # VBox([fig, out])\n",
    "    # return fig\n",
    "\n",
    "\n",
    "def plot_square(data_df, x, y, reasons, height=800, width=800):\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(height=height, width=width, hovermode='closest')\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_df[x], y=data_df[y], text=data_df['SCENE_ID'], \n",
    "        name='Full', mode='markers', showlegend=False,\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color='rgba(0, 0, 0, 0)',\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    for reason in reasons:\n",
    "        if len(data_df.loc[data_df['SKIPPED']==reason, y]) == 0:\n",
    "            continue\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data_df.loc[data_df['SKIPPED']==reason, x], \n",
    "            y=data_df.loc[data_df['SKIPPED']==reason, y], \n",
    "            text=data_df.loc[data_df['SKIPPED']==reason, 'SCENE_ID'],\n",
    "            name=reason, mode='markers', hoverinfo='skip',\n",
    "            marker=dict(\n",
    "                symbol='circle' if reason == 'None' else 'triangle-up',\n",
    "                size=8,\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    out = Output()\n",
    "    @out.capture(clear_output=True)\n",
    "    def do_click(trace, points, selector):\n",
    "        if points.point_inds:\n",
    "            pyperclip.copy(data_df.SCENE_ID.iloc[points.point_inds[0]])\n",
    "            print(f'{data_df.SCENE_ID.iloc[points.point_inds[0]]}')\n",
    "    fig.data[0].on_click(do_click)\n",
    "    return fig, out\n",
    "    # VBox([fig, out])\n",
    "    # return fig\n",
    "\n",
    "\n",
    "wrs2_stats_df = stats_df[stats_df['WRS2'] == 'p042r032'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c11864-5196-4025-b1eb-9639fa03521a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512945f-1ed0-46d8-8cf5-4d633a2684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 1200\n",
    "print_count = 10\n",
    "\n",
    "start_year = 2017\n",
    "end_year = 2026\n",
    "\n",
    "years = list(range(start_year, end_year + 1))\n",
    "months = []\n",
    "test_years = []\n",
    "# test_years = [2025, 2026]\n",
    "\n",
    "#outlier_band = 'UNMASKED_SR'\n",
    "outlier_band = 'UNMASKED_TOA'\n",
    "#outlier_band = 'UNMASKED_LST'  \n",
    "\n",
    "# z_score_min = 6\n",
    "# z_score_min = 5\n",
    "# z_score_min = 4\n",
    "# z_score_min = 3\n",
    "z_score_min = 2.5\n",
    "# z_score_min = 2\n",
    "z_score_max = 100\n",
    "\n",
    "\n",
    "count_threshold_pct_min = 0\n",
    "count_threshold_pct_max = 100\n",
    "\n",
    "# outlier_cols = [\n",
    "#     #'UNMASKED_SR',\n",
    "#     'UNMASKED_TOA',\n",
    "#     #'UNMASKED_LST',\n",
    "#     # 'MORAN_1K',\n",
    "#     # 'CLOUD_COVER_LAND',\n",
    "#     # 'CLOUD_COUNT_RATIO',\n",
    "#     # 'SNOW_COUNT_RATIO',\n",
    "#     # 'UNMASKED_PIXELS',\n",
    "#     # 'ACCA_PIXELS',\n",
    "#     # 'SATURATED_PIXELS',\n",
    "#     # 'ACCA_COUNT_RATIO',\n",
    "#     # 'SATURATED_COUNT_RATIO',\n",
    "#     'MONTH',\n",
    "# ]\n",
    "\n",
    "\n",
    "# Read in the scene skip list\n",
    "scene_skip_url = '../v2p1_denmark.csv'\n",
    "# scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "# Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # Add the cloudscore images to the skip list\n",
    "# scene_skip_list = set(scene_skip_list + scene_cloudscore_list)\n",
    "\n",
    "\n",
    "new_skip_scenes = []\n",
    "new_skip_count = 0\n",
    "\n",
    "wrs2_i = 0\n",
    "\n",
    "# for wrs2 in reversed(wrs2_tiles):\n",
    "# for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "    if wrs2_i >= print_count:\n",
    "        break\n",
    "    if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "        continue\n",
    "    # if wrs2 not in california_wrs2_list:\n",
    "    #     continue\n",
    "    # if int(wrs2[1:4]) not in range(30, 48):\n",
    "    #     continue\n",
    "    #if int(wrs2[5:8]) not in range(27, 50):\n",
    "    #    continue\n",
    "    # if wrs2 != 'p031r026':\n",
    "    #     continue    print(wrs2)\n",
    "\n",
    "    wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "\n",
    "    # Remove the high CLOUD_COVER_LAND scenes before computing outliers\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "\n",
    "    # Remove the high CLOUD_COUNT_RATIO scenes before computing outliers\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < 90]\n",
    "\n",
    "    # Should the known outliers be removed before checking for outliers?\n",
    "    # Remove the high CLOUD_COUNT_RATIO scenes before computing outliers\n",
    "    wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "\n",
    "    if years:\n",
    "        wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['YEAR'].isin(years)]\n",
    "    \n",
    "    if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "        continue\n",
    "    print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "    wrs2_path = int(wrs2[1:4])\n",
    "    wrs2_row = int(wrs2[5:8])\n",
    "    wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "    wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "    wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'  \n",
    "\n",
    "    # # # DEADBEEF - Trying to debug why UNMASKED_LST wasn't working\n",
    "    # print(wrs2_stats_df.loc[['UNMASKED_LST'].isna(), 'SCENE_ID'])\n",
    "    # # wrs2_i += 1\n",
    "    # # continue\n",
    "\n",
    "    # Fit an annual function to the data\n",
    "    # Initial guesses for the parameters\n",
    "    wrs2_stats_df.sort_values('DATE', ascending=True, inplace=True)\n",
    "    wrs2_stats_df['DAYS'] = (pd.to_datetime(wrs2_stats_df['DATE']) - datetime.datetime(start_year, 1, 1)).dt.days\n",
    "    days = wrs2_stats_df['DAYS'].values\n",
    "    data = wrs2_stats_df[outlier_band].values\n",
    "    p0 = [\n",
    "        (np.max(data) - np.min(data)) / 2,            # Amplitude\n",
    "        0,                                            # Phase\n",
    "        (data[-1] - data[0]) / (days[-1] - days[0]),  # Slope\n",
    "        np.mean(data)                                 # Intercept\n",
    "    ]\n",
    "\n",
    "    def annual_fit(x, A, phi, m, C):\n",
    "        \"\"\"\n",
    "        Model function for a time series with an annual frequency and a linear trend.\n",
    "    \n",
    "        Args:\n",
    "            x (array): Time data in days.\n",
    "            A (float): Amplitude of the annual cycle.\n",
    "            phi (float): Phase shift of the sine wave.\n",
    "            m (float): Slope of the linear trend.\n",
    "            C (float): Intercept (offset) of the linear trend.\n",
    "        \"\"\"\n",
    "        return A * np.sin(2 * np.pi * x / 365 + phi) + m * x + C\n",
    "        \n",
    "    popt, pcov = curve_fit(annual_fit, days, data, p0=p0)\n",
    "    fitted_amplitude, fitted_phase, fitted_slope, fitted_intercept = popt\n",
    "    fit_curve = annual_fit(days, *popt)\n",
    "    wrs2_stats_df['ERROR'] = wrs2_stats_df[outlier_band].values - annual_fit(wrs2_stats_df['DAYS'], *popt)\n",
    "\n",
    "    # # Plot the original data and the fitted curve\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.scatter(days, data, label='Original Data', alpha=0.6)\n",
    "    # plt.plot(days, fit_curve, label='Fitted Curve', color='red', linewidth=2)\n",
    "    # plt.title('Time Series with Annual Frequency Fit')\n",
    "    # plt.xlabel('Time (years)')\n",
    "    # plt.ylabel('Data')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    # display(VBox(plot_timeseries(wrs2_stats_df, x='DATE', y='ERROR', reasons=['None'])))\n",
    "\n",
    "    \n",
    "    # # SciPy Outlier detection\n",
    "    # # IsolationForest\n",
    "    # model = IsolationForest(random_state=42)\n",
    "    # wrs2_stats_df['OUTLIER_IF_SCORE'] = model.fit_predict(wrs2_stats_df[outlier_cols])\n",
    "\n",
    "    # # OneClassSVM\n",
    "    # # nu is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.\n",
    "    # model = OneClassSVM(nu=0.1) \n",
    "    # wrs2_stats_df['OUTLIER_SVM_SCORE'] = model.fit_predict(wrs2_stats_df[outlier_cols])\n",
    "    \n",
    "    # outlier_mask = (\n",
    "    #     ((wrs2_stats_df['OUTLIER_IF_SCORE'] <= 0) | (wrs2_stats_df['OUTLIER_SVM_SCORE'] <= 0))\n",
    "    #     & (wrs2_stats_df['SKIPPED'] == 'None')\n",
    "    # )\n",
    "    # outlier_df = wrs2_stats_df[outlier_mask].copy()\n",
    "\n",
    "\n",
    "    # Identify outliers based on z-scores of the SR and LST bands\n",
    "    # Test out different combinations of bands and z-score values\n",
    "    z_scores_error = stats.zscore(wrs2_stats_df['ERROR'])\n",
    "    if z_score_max:\n",
    "        outlier_mask = (np.abs(z_scores_error) >= z_score_min) & (np.abs(z_scores_error) < z_score_max)\n",
    "    else:\n",
    "        outlier_mask = np.abs(z_scores_error) >= z_score_min\n",
    "    # Test out masking based on combinationso f z-score masks\n",
    "    # outlier_mask = (np.abs(stats.zscore(wrs2_stats_df['UNMASKED_LST'])) > 3) & (np.abs(stats.zscore(wrs2_stats_df['UNMASKED_SR'])) > 3)\n",
    "    \n",
    "    outlier_df = wrs2_stats_df[outlier_mask].copy()\n",
    "    if outlier_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Filter on the overall cloud count ratio\n",
    "    outlier_df = outlier_df[outlier_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "    outlier_df = outlier_df[outlier_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "\n",
    "    # # Apply date filtering to outliers\n",
    "    if months:\n",
    "        outlier_df = outlier_df[outlier_df['MONTH'].isin(months)]\n",
    "    if test_years:\n",
    "        outlier_df = outlier_df[outlier_df['YEAR'].isin(test_years)]  \n",
    "\n",
    "\n",
    "    # # TODO: Make this a function    \n",
    "    # # Apply above and below or above or below filters\n",
    "    # filter_keep_list = []\n",
    "    # for i, row in outlier_df.iterrows():\n",
    "    #     scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "    #     above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "    #     above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "    #     if len(above_stats_df):\n",
    "    #         above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "    #     else:\n",
    "    #         above_cloud_pct = None\n",
    "            \n",
    "    #     below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "    #     below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "    #     if len(below_stats_df):\n",
    "    #         below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "    #     else:\n",
    "    #         below_cloud_pct = None\n",
    "\n",
    "    #     # Only show scenes that have above & below both skipped or None\n",
    "    #     if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "    #         ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "    #         continue   \n",
    "\n",
    "    #     # # Only show scenes that have either above & below skipped or None\n",
    "    #     # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) and \n",
    "    #     #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "    #     #     continue   \n",
    "\n",
    "    #     filter_keep_list.append(scene_id)\n",
    "\n",
    "    # if filter_keep_list:\n",
    "    #     outlier_df = outlier_df[outlier_df['SCENE_ID'].str.upper().isin(filter_keep_list)]\n",
    "    # else:\n",
    "    #     continue\n",
    "\n",
    "\n",
    "    if outlier_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Plot the timeseries as the last step before cycling through the scenes\n",
    "    wrs2_stats_df.loc[outlier_mask, 'SKIPPED'] = 'Outlier'\n",
    "    display(VBox(plot_timeseries(wrs2_stats_df, x='DATE', y=outlier_band, reasons=['None', 'Outlier'])))\n",
    "    # display(VBox(plot_timeseries(wrs2_stats_df, x='DATE', y='SATURATED_PIXELS', reasons=['None', 'Outlier'])))\n",
    "    # display(VBox(plot_timeseries(wrs2_stats_df, x='DATE', y='SATURATED_COUNT_RATIO', reasons=['None', 'Outlier'])))\n",
    "\n",
    "    wrs2_skip_scenes = []\n",
    "    wrs2_skip_count = 0\n",
    "\n",
    "    # for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "    # for i, row in wrs2_stats_df.iterrows():\n",
    "    for i, row in outlier_df.iterrows():\n",
    "\n",
    "        scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "        above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "        above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "        if len(above_stats_df):\n",
    "            above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            above_cloud_pct = None\n",
    "            \n",
    "        below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "        below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "        if len(below_stats_df):\n",
    "            below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            below_cloud_pct = None\n",
    "            \n",
    "        landsat_type = scene_id.split('_')[0].upper()\n",
    "        landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "        landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "        # Landsat true color image\n",
    "        landsat_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "            .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        # Landsat true color with Fmask\n",
    "        fmask_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "            .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "            .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        print('#'*80)\n",
    "        print(\n",
    "            f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "            f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "            f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "        )\n",
    "        ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "        # Show the images above and below the target wrs2\n",
    "        above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "        above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            above_url = (\n",
    "                above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            above_url = None\n",
    "            \n",
    "        below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "        below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            below_url = (\n",
    "                below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            below_url = None\n",
    "        \n",
    "        above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "        below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "        if above_url and below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "        elif above_url:\n",
    "            print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([above_url], img_width=image_size)\n",
    "        elif below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "            ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "        wrs2_skip_scenes.append(scene_id)\n",
    "        wrs2_skip_count += 1\n",
    "        if wrs2_skip_count >= print_count:\n",
    "            break\n",
    "\n",
    "    if wrs2_skip_scenes:\n",
    "        wrs2_i += 1\n",
    "        for scene_id in wrs2_skip_scenes:\n",
    "            print(scene_id)\n",
    "        new_skip_scenes.extend(wrs2_skip_scenes)\n",
    "    \n",
    "    # break\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be673be0-67da-4623-8089-21a399253227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4721243-1ab2-4321-99cb-de71e7098712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
