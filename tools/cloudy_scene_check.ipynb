{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f554972-a84a-441c-8565-589f338a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openet.core\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import ipyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f809b90-fa61-43b6-b3c3-cb365ec6ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(\n",
    "    project='ee-cmorton',\n",
    "    opt_url='https://earthengine-highvolume.googleapis.com'\n",
    ")\n",
    "\n",
    "stats_ws = os.path.join(os.getcwd(), 'stats')\n",
    "if not os.path.isdir(stats_ws):\n",
    "    os.makedirs(stats_ws)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f175af-c860-4ddf-b9d1-72224e0f7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the list of WRS2 tiles from the SSEBop collection\n",
    "# wrs2_list = sorted(\n",
    "#     # ee.ImageCollection('projects/openet/assets/intercomparison/ssebop/landsat/c02/v0p2p6')\n",
    "#     ee.ImageCollection('projects/usgs-gee-nhm-ssebop/assets/ssebop/landsat/c02')\n",
    "#     .filterDate('2020-01-01', '2024-01-01')\n",
    "#     .aggregate_histogram('wrs2_tile').keys().getInfo(),\n",
    "#     reverse=True\n",
    "# )\n",
    "# wrs2_list = wrs2_list + ['p018r028']\n",
    "# # pprint.pprint(wrs2_list)\n",
    "\n",
    "wrs2_skip_list = [\n",
    "    'p010r027', 'p010r030', \n",
    "]\n",
    "\n",
    "wrs2_list = sorted(\n",
    "    ee.FeatureCollection('projects/openet/assets/features/wrs2/custom')\n",
    "    .filterBounds(ee.Geometry.BBox(-124, 26, -68, 50))\n",
    "    .filter(ee.Filter.inList('wrs2_tile', wrs2_skip_list).Not())\n",
    "    .aggregate_histogram('wrs2_tile').keys().getInfo(),\n",
    "    reverse=True\n",
    ")\n",
    "print(len(wrs2_list))\n",
    "\n",
    "\n",
    "ocean_wrs2_list = [\n",
    "    'p048r027', 'p047r031', 'p047r030', 'p047r029', 'p046r033', \n",
    "    'p045r034', 'p044r035', 'p043r036', 'p041r037', 'p040r038', \n",
    "    'p038r041', 'p038r040',\n",
    "    'p025r040', 'p024r040', 'p024r027', 'p023r040', \n",
    "    'p023r027', 'p022r040', 'p021r040', 'p020r029',\n",
    "    'p017r041', 'p016r038', 'p015r040', 'p015r037', \n",
    "    'p013r033', 'p012r032', 'p011r031', 'p011r030', \n",
    "]\n",
    "\n",
    "california_wrs2_list = [\n",
    "    'p038r036', 'p038r037', \n",
    "    'p039r035', 'p039r036', 'p039r037',\n",
    "    'p040r034', 'p040r035', 'p040r036', 'p040r037',\n",
    "    'p041r034', 'p041r035', 'p041r036', 'p041r037',\n",
    "    'p042r033', 'p042r034', 'p042r035', 'p042r036',\n",
    "    'p043r031', 'p043r032', 'p043r033', 'p043r034', 'p043r035',\n",
    "    'p044r031', 'p044r032', 'p044r033', 'p044r034',\n",
    "    'p045r031', 'p045r032', 'p045r033',\n",
    "    'p046r031', 'p046r032', 'p047r031',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58b512-59c5-473d-8cd5-447317382df8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "land_mask = ee.Image('projects/openet/assets/features/water_mask').Not()\n",
    "# Apply the NLCD/NALCMS water mask (anywhere it is water, set the ocean mask \n",
    "land_mask = land_mask.where(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).eq(18), 0)\n",
    "# land_mask = land_mask.And(ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\").unmask(18).neq(18))\n",
    "# # land_mask = ee.Image('projects/openet/assets/meteorology/conus404/ancillary/land_mask')\n",
    "\n",
    "# etf_coll_id = 'projects/openet/assets/ssebop/conus/gridmet/landsat/c02'\n",
    "etf_coll_id = 'projects/usgs-gee-nhm-ssebop/assets/ssebop/landsat/c02'\n",
    "# etf_coll_id = 'projects/openet/assets/intercomparison/ssebop/landsat/c02/v0p2p6'\n",
    "band_name = 'et_fraction'\n",
    "\n",
    "rgb_bands = {\n",
    "    'LT04': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LT05': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LE07': ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "    'LC08': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "    'LC09': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
    "}\n",
    "\n",
    "# 0 - white, 1 - no fill (green), 2 - shadow (dark blue), 3 - snow (light blue), 4 - cloud (light gray), 5 - water (purple), 6 - ocean mask\n",
    "fmask_palette = \"ffffff, 9effa1, blue, 00aff2, dddddd, purple, bfbfbf\"\n",
    "fmask_max = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91566e57-a06c-4da0-8c28-8e91aa2db0ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Intercomparison sites and dates\n",
    "\n",
    "# sites_csv = '/Users/Charles.Morton@dri.edu/Projects/openet-tools/intercomparison/master_flux_station_list.csv'\n",
    "# sites_df = pd.read_csv(sites_csv)\n",
    "\n",
    "# interp_days = 32\n",
    "# site_keep_list = []\n",
    "# wrs2_delimiter = ';'\n",
    "\n",
    "# # Hardcoding the sites CSV field names for now\n",
    "# start_field = 'START_DATE'\n",
    "# end_field = 'END_DATE'\n",
    "# site_field = 'SITE_ID'\n",
    "# lat_field = 'LATITUDE'\n",
    "# lon_field = 'LONGITUDE'\n",
    "# wrs2_field = 'WRS2_TILES'\n",
    "\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# # Group the date ranges by WRS2 tile\n",
    "# # print(f'\\nGrouping overlapping dates')\n",
    "# wrs2_dates = collections.defaultdict(list)\n",
    "# wrs2_sites = collections.defaultdict(list)\n",
    "# for (site_i, site) in sites_df.iterrows():\n",
    "#     # print(site_i)\n",
    "#     # print(site)\n",
    "#     if site['RANDOM_SELECTION'] not in [0, 1]:\n",
    "#         # print('  Unsupported RANDOM_SELECTION value')\n",
    "#         input('ENTER')\n",
    "#         continue\n",
    "#     # if site['RANDOM_SELECTION'] != 1:\n",
    "#     #     continue\n",
    "#     if site_keep_list and site.loc[site_field] not in site_keep_list:\n",
    "#         # print('  Site not in keep list - skipping')\n",
    "#         continue\n",
    "\n",
    "#     # Include all sites in INI file, even those outside the date range\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_sites[wrs2.strip()].append([\n",
    "#             round(site.loc[lon_field], 6), round(site.loc[lat_field], 6)\n",
    "#         ])\n",
    "\n",
    "#     start_dt = datetime.datetime.strptime(site.loc[start_field], '%Y-%m-%d')\n",
    "#     end_dt = datetime.datetime.strptime(site.loc[end_field], '%Y-%m-%d')\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # If start/end dates are within N gap days of the start/end of the month\n",
    "#     #   consider it a \"full\" month\n",
    "#     gap_days = 5\n",
    "#     # print('  Snapping start date to month')\n",
    "#     month_start_dt = datetime.datetime(start_dt.year, start_dt.month, 1)\n",
    "#     if (start_dt - month_start_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         start_dt = month_start_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         start_dt = month_start_dt + relativedelta(months=1)\n",
    "\n",
    "#     # print('  Snapping end date to month')\n",
    "#     month_end_dt = end_dt + relativedelta(months=1)\n",
    "#     month_end_dt = datetime.datetime(month_end_dt.year, month_end_dt.month, 1)\n",
    "#     month_end_dt = month_end_dt - relativedelta(days=1)\n",
    "#     if (month_end_dt - end_dt).days <= gap_days:\n",
    "#         # print('    full month')\n",
    "#         end_dt = month_end_dt\n",
    "#     else:\n",
    "#         # print('    not full month')\n",
    "#         end_dt = (month_end_dt + relativedelta(days=1) -\n",
    "#                   relativedelta(months=1) - relativedelta(days=1))\n",
    "#     # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#     # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     if interp_days > 0:\n",
    "#         # Buffer the date ranges by the interpolate days value if set\n",
    "#         # print('  Buffering start/end dates')\n",
    "#         start_dt = start_dt - datetime.timedelta(days=interp_days)\n",
    "#         end_dt = end_dt + datetime.timedelta(days=interp_days)\n",
    "#         # print(f'  Start Date: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End Date:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "#     # CM - Changing conditionals to get single date ranges to work\n",
    "#     # if end_dt <= start_dt or start_dt >= end_dt:\n",
    "#     if end_dt < start_dt or start_dt > end_dt:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print('  Date range outside min/max, skipping')\n",
    "#         continue\n",
    "#     else:\n",
    "#         # print(f'  Start: {start_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         # print(f'  End:   {end_dt.strftime(\"%Y-%m-%d\")}')\n",
    "#         pass\n",
    "\n",
    "#     for wrs2 in site.loc[wrs2_field].split(wrs2_delimiter):\n",
    "#         wrs2_dates[wrs2.strip()].append([start_dt, end_dt])\n",
    "\n",
    "# # pprint.pprint(wrs2_dates)\n",
    "\n",
    "# # Merge the date ranges that overlap\n",
    "# print(f'\\nMerging overlapping dates')\n",
    "# merged_dates = {}\n",
    "# for wrs2, dates in sorted(wrs2_dates.items()):\n",
    "#     # print(f'  {wrs2}')\n",
    "#     # pprint.pprint(sorted(dates))\n",
    "\n",
    "#     # Push the first interval on to the stack\n",
    "#     merged_dates[wrs2] = [sorted(dates)[0]]\n",
    "\n",
    "#     # Only check for overlapping ranges if there is more than 1 range\n",
    "#     if len(dates) == 1:\n",
    "#         continue\n",
    "\n",
    "#     for d in sorted(dates)[1:]:\n",
    "#         # If the current date range doesn't overlap, add it to the stack\n",
    "#         if d[0] > merged_dates[wrs2][-1][1]:\n",
    "#             merged_dates[wrs2].append(d)\n",
    "#         # If the ranges overlap and the end date is later,\n",
    "#         #   update the end time of the stack value\n",
    "#         elif ((d[0] <= merged_dates[wrs2][-1][1]) and\n",
    "#               (d[1] > merged_dates[wrs2][-1][1])):\n",
    "#             merged_dates[wrs2][-1][1] = d[1]\n",
    "\n",
    "# # pprint.pprint(merged_dates)\n",
    "\n",
    "# # # CGM - Splitting by year for DisALEXI is not needed if the NLCD\n",
    "# # #   is set to the image collection instead of the image\n",
    "# # # For DisALEXI split the date ranges by year after merging\n",
    "# # # For other models, index by the first year in the range\n",
    "# # # This may be functionality we will want for other models later\n",
    "# # year_dates = collections.defaultdict(dict)\n",
    "# # # if model in ['DISALEXI_TAIR_10K', 'DISALEXI_TAIR_1K', 'DISALEXI', 'DISALEXI_TAIR_DIRECT']:\n",
    "# # #     for wrs2, dates in merged_dates.items():\n",
    "# # #         # split_dates = {}\n",
    "# # #         for date_i, date in enumerate(dates):\n",
    "# # #             for year in range(date[0].year, date[1].year+1):\n",
    "# # #                 year_date = [\n",
    "# # #                     max(date[0], datetime.datetime(year, 1, 1)),\n",
    "# # #                     min(date[1], datetime.datetime(year, 12, 31)),\n",
    "# # #                 ]\n",
    "# # #                 try:\n",
    "# # #                     year_dates[wrs2][year].append(year_date)\n",
    "# # #                 except:\n",
    "# # #                     year_dates[wrs2][year] = [year_date]\n",
    "# # # else:\n",
    "# # for wrs2, dates in merged_dates.items():\n",
    "# #     year_dates[wrs2][dates[0][0].year] = dates\n",
    "\n",
    "# # pprint.pprint(year_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdaea1-e7ee-4ed3-a392-4f44f02bd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmask(landsat_img):\n",
    "    # Add the fmask image on top of the true color image\n",
    "    qa_img = landsat_img.select('QA_PIXEL')\n",
    "    fill_mask = qa_img.bitwiseAnd(1).neq(0)                  # bits: 0\n",
    "    dilate_mask = qa_img.rightShift(1).bitwiseAnd(1).neq(0)  # bits: 1\n",
    "    cirrus_mask = qa_img.rightShift(2).bitwiseAnd(1).neq(0)  # bits: 2\n",
    "    cloud_mask = qa_img.rightShift(3).bitwiseAnd(1).neq(0)   # bits: 3\n",
    "    shadow_mask = qa_img.rightShift(4).bitwiseAnd(1).neq(0)  # bits: 4\n",
    "    snow_mask = qa_img.rightShift(5).bitwiseAnd(1).neq(0)    # bits: 5\n",
    "    clear_mask = qa_img.rightShift(6).bitwiseAnd(1).neq(0)   # bits: 6\n",
    "    water_mask = qa_img.rightShift(7).bitwiseAnd(1).neq(0)   # bits: 7\n",
    "    # cloud_conf = qa_img.rightShift(8).bitwiseAnd(3)          # bits: 8, 9\n",
    "    # shadow_conf = qa_img.rightShift(10).bitwiseAnd(3)        # bits: 10, 11\n",
    "    # snow_conf = qa_img.rightShift(12).bitwiseAnd(3)          # bits: 12, 13\n",
    "    # cirrus_conf = qa_img.rightShift(14).bitwiseAnd(3)        # bits: 14, 15\n",
    "\n",
    "    # Saturated pixels\n",
    "    # Flag as saturated if any of the RGB bands are saturated\n",
    "    #   or change .gt(0) to .gt(7) to flag if all RGB bands are saturated\n",
    "    # Comment out rightShift line to flag if saturated in any band\n",
    "    bitshift = ee.Dictionary({'LANDSAT_4': 0, 'LANDSAT_5': 0, 'LANDSAT_7': 0, 'LANDSAT_8': 1, 'LANDSAT_9': 1});\n",
    "    saturated_mask = (\n",
    "        landsat_img.select('QA_RADSAT')\n",
    "        .rightShift(ee.Number(bitshift.get(ee.String(landsat_img.get('SPACECRAFT_ID'))))).bitwiseAnd(7)\n",
    "        .gt(0)\n",
    "    )\n",
    "    \n",
    "    # Old \"Fmask\" style image\n",
    "    fmask_img = (\n",
    "        qa_img.multiply(0)\n",
    "        .where(landsat_img.select(['SR_B4']).mask().eq(0), 1)\n",
    "        # .where(saturated_mask, 6)\n",
    "        .where(water_mask, 5)\n",
    "        .where(shadow_mask, 2)\n",
    "        .where(snow_mask, 3)\n",
    "        .where(cloud_mask.Or(dilate_mask).Or(cirrus_mask), 4)\n",
    "        # .add(shadow_mask.multiply(2))\n",
    "        # .add(snow_mask.multiply(3))\n",
    "        # .add(cloud_mask.Or(dilate_mask).Or(cirrus_mask).multiply(4))\n",
    "        # .add(cloud_mask.Or(dilate_mask).multiply(4))\n",
    "        # .add(cloud_mask.And(cloud_conf).multiply(4))\n",
    "        # .add(water_mask.multiply(5))\n",
    "    )\n",
    "    \n",
    "    return fmask_img.updateMask(fmask_img.neq(0)).rename(['fmask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784957e0-dec8-41ca-a833-9776631cfa2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the scene skip list file\n",
    "skip_path = '../v2p1.csv'\n",
    "print(f'\\n{skip_path}')\n",
    "\n",
    "with open(skip_path, 'r') as csv_f:\n",
    "    scene_skip_lines = csv_f.readlines()\n",
    "scene_skip_header = scene_skip_lines.pop(0)\n",
    "\n",
    "# Drop the comments and empty lines\n",
    "scene_skip_lines = [line.strip() for line in scene_skip_lines if line.strip() and line[0] != '#']\n",
    "\n",
    "# Sort by date then by tile\n",
    "scene_skip_lines = sorted(scene_skip_lines, key=lambda x:x.split(',')[0].split('_')[-1] + '_' + x.split(',')[0].split('_')[-2])\n",
    "\n",
    "# Identify duplicate scene IDs (as opposed to duplicate lines)\n",
    "# Note, this block is not removing any lines, just printing\n",
    "print('Duplicate Scene IDs:')\n",
    "\n",
    "if len({l.split(',')[0] for l in scene_skip_lines}) != len(scene_skip_lines):\n",
    "    for item, count in collections.Counter([l.split(',')[0] for l in scene_skip_lines]).items():\n",
    "        if count > 1:\n",
    "            print(item)\n",
    "\n",
    "# Identify lines with no reason\n",
    "print('\\nMissing reason Scene IDs:')\n",
    "for l in scene_skip_lines:\n",
    "    if ',' not in l:\n",
    "        print(l)\n",
    "    elif l.split(',')[1].strip() == '':\n",
    "        print(l)\n",
    "    elif len(l.split(',')) > 2:\n",
    "        print(l)\n",
    "\n",
    "# # Identify duplicate lines (not duplicate SCENE IDs)\n",
    "# if len({line for line in scene_skip_lines}) != len(scene_skip_lines):\n",
    "#     print('Duplicate Lines:')\n",
    "#     for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#         if count > 1:\n",
    "#             print(item)\n",
    "# \n",
    "#     # # Uncomment to have the tool remove duplicate lines\n",
    "#     # scene_remove_lines = []\n",
    "#     # for item, count in collections.Counter(scene_skip_lines).items():\n",
    "#     #     if count > 1:\n",
    "#     #         scene_remove_lines.append(item)\n",
    "#     #         # print(item)\n",
    "#      \n",
    "#     # # Does this only remove the first one?\n",
    "#     # if scene_remove_lines:\n",
    "#     #     print(f'Removing {len(scene_remove_lines)} duplicate lines in file')\n",
    "#     #     for line in scene_remove_lines:\n",
    "#     #         print(line)\n",
    "#     #         scene_skip_lines.remove(line)\n",
    "# \n",
    "# # Then recheck for duplicate SCENE_IDs (but different notes or dates)\n",
    "# scenes = {line.split(',')[0] for line in scene_skip_lines}           \n",
    "# if len(scenes) != len(scene_skip_lines):\n",
    "#     print('Duplicate scene IDs still in file')\n",
    "    \n",
    "print('\\nWriting updated scene skip list CSV')\n",
    "with open(skip_path.replace('.csv', '_sorted.csv'), 'w') as csv_f:\n",
    "    csv_f.write(scene_skip_header)\n",
    "    for i, line in enumerate(scene_skip_lines):\n",
    "        csv_f.write(line + '\\n')\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb643-0c14-4dec-90e5-e392adfd47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the EEMETRIC skip list by merging the EEMETRIC error list and the full skip list\n",
    "scene_skip_path = '../v2p1_sorted.csv'\n",
    "eemetric_error_path = '../v2p1_eemetric_error.csv'\n",
    "eemetric_skip_path = '../v2p1_eemetric.csv'\n",
    "\n",
    "with open(eemetric_error_path, 'r') as csv_f:\n",
    "    eemetric_skip_lines = csv_f.readlines()\n",
    "    \n",
    "with open(scene_skip_path, 'r') as csv_f:\n",
    "    scene_skip_lines = csv_f.readlines()\n",
    "\n",
    "print('\\nWriting eemetric scene skip list CSV')\n",
    "with open(eemetric_skip_path, 'w') as csv_f:\n",
    "    for i, line in enumerate(eemetric_skip_lines):\n",
    "        csv_f.write(line)\n",
    "    csv_f.write('\\n')\n",
    "    for i, line in enumerate(scene_skip_lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        csv_f.write(line)\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7135e03-10dd-4577-9b6d-11b1383f5feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61faa3e-a250-486c-b34f-cb4b57fe6971",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Print scenes with high masked count percentages\n",
    "#count_threshold_pct_min = 80\n",
    "#count_threshold_pct_min = 89\n",
    "#count_threshold_pct_max = 101\n",
    "\n",
    "count_threshold_pct_min = 75\n",
    "count_threshold_pct_max = 101\n",
    "\n",
    "start_year = 1984\n",
    "#start_year = 2003\n",
    "#start_year = 2015\n",
    "#start_year = 2025\n",
    "end_year = 2025\n",
    "years = list(range(start_year, end_year + 1))\n",
    "\n",
    "print_count = 10\n",
    "# image_size = 700\n",
    "# image_size = 900\n",
    "# image_size = 1024\n",
    "image_size = 1400\n",
    "\n",
    "# Read in the scene skip list\n",
    "scene_skip_url = '../v2p1.csv'\n",
    "# scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "print('Reading image stats CSV files')\n",
    "stats_df_list = []\n",
    "for wrs2_tile in wrs2_list:\n",
    "    # if int(wrs2_tile[1:4]) not in range(10, 25):\n",
    "    #     continue\n",
    "        \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "        if not os.path.isfile(wrs2_stats_path):\n",
    "            # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "            continue\n",
    "        try:\n",
    "            wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "        except Exception as e:\n",
    "            print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "            continue\n",
    "        if wrs2_stats_df.empty:\n",
    "            continue\n",
    "        wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "        wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "        stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# Skip the Landsat 7 scenes in 2023\n",
    "l7_2022_mask = (\n",
    "    (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "    (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    ")\n",
    "stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# Compute the ratios\n",
    "# stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SHADOW_COUNT_RATIO'] = stats_df['SHADOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "stats_df['MASKED_PIXELS'] = (\n",
    "    stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "    + stats_df['SHADOW_PIXELS']\n",
    "    + stats_df['SNOW_PIXELS']\n",
    "    # + stats_df['WATER_PIXELS']\n",
    "    + stats_df['ACCA_PIXELS']\n",
    "    # + stats_df['SATURATED_PIXELS']\n",
    ")\n",
    "stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# Work through the tiles based on which ones already have the most skipped scenes\n",
    "wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# wrs2_tiles = ['']\n",
    "\n",
    "new_skip_scenes = []\n",
    "new_skip_count = 0\n",
    "\n",
    "wrs2_i = 0\n",
    "\n",
    "# for wrs2 in reversed(wrs2_tiles):\n",
    "# for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# for wrs2 in sorted(wrs2_tiles):\n",
    "for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "    if wrs2_i >= 20:\n",
    "        break\n",
    "    if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "        continue\n",
    "    # if california_wrs2_list and (wrs2 not in california_wrs2_list) and wrs2 not in ['p042r033']:\n",
    "    #     continue\n",
    "    # if int(wrs2[5:8]) >= 29:\n",
    "    #     continue\n",
    "    \n",
    "    wrs2_path = int(wrs2[1:4])\n",
    "    wrs2_row = int(wrs2[5:8])\n",
    "    wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "    wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "    wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "    wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "    # Applying skip list here so that main stats DF has all scenes\n",
    "    wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "    wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "    \n",
    "    # Only check winter scenes\n",
    "    #wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([11, 12, 1, 2, 3])]\n",
    "    #wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([10, 11, 12, 1, 2, 3, 4])]\n",
    "    \n",
    "    # Filter on the overall cloud count ratio\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "    wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "    # # Filter on the CLOUD_COVER_LAND property\n",
    "    wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "    #wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 60]\n",
    "    #wrs2_stats_df.sort_values('CLOUD_COVER_LAND', ascending=False, inplace=True)\n",
    "\n",
    "    if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "        continue\n",
    "    print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "    new_skip_scenes = []\n",
    "    new_skip_count = 0\n",
    "    \n",
    "    # for i, row in wrs2_stats_df.iterrows():\n",
    "    for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "\n",
    "        scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "        above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "        above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "        if len(above_stats_df):\n",
    "            above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            above_cloud_pct = None\n",
    "            \n",
    "        below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "        below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "        if len(below_stats_df):\n",
    "            below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "        else:\n",
    "            below_cloud_pct = None\n",
    "\n",
    "        # # Only show scenes that have above & below both skipped or None\n",
    "        # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "        #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "        #     continue   \n",
    "        \n",
    "        landsat_type = scene_id.split('_')[0].upper()\n",
    "        landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "        landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "        # Landsat true color image\n",
    "        landsat_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "            .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        # Landsat true color with Fmask\n",
    "        fmask_url = (\n",
    "            landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "            .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "            .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "        )\n",
    "    \n",
    "        print('#'*80)\n",
    "        print(\n",
    "            f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "            f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "            f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "        )\n",
    "        ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "        # Show the images above and below the target wrs2\n",
    "        above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "        above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            above_url = (\n",
    "                above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            above_url = None\n",
    "            \n",
    "        below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "        below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "        below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "        try:\n",
    "            below_url = (\n",
    "                below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "                .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "                .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "            )\n",
    "        except:\n",
    "            below_url = None\n",
    "    \n",
    "        above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "        below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "        if above_url and below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "        elif above_url:\n",
    "            print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "            ipyplot.plot_images([above_url], img_width=image_size)\n",
    "        elif below_url:\n",
    "            print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "            ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "        new_skip_scenes.append(scene_id)\n",
    "        new_skip_count += 1\n",
    "        if new_skip_count >= print_count:\n",
    "            break\n",
    "\n",
    "    if new_skip_scenes:\n",
    "        wrs2_i += 1\n",
    "        for scene_id in new_skip_scenes:\n",
    "            print(scene_id)     \n",
    "\n",
    "# print('\\nNew Skip Scenes')\n",
    "# if new_skip_scenes:\n",
    "#     for scene_id in new_skip_scenes:\n",
    "#         print(scene_id)\n",
    "\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379611de-9df1-4239-ab11-5f426bc15aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b94e93-9a90-45fe-bd78-dcea5bbce670",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes with high numbers of unmasked clouds\n",
    "# #count_threshold_pct_min = 20\n",
    "# #count_threshold_pct_min = 10\n",
    "# count_threshold_pct_min = 5\n",
    "# #count_threshold_pct_min = 2\n",
    "# #count_threshold_pct_min = 1\n",
    "# count_threshold_pct_max = 101\n",
    "\n",
    "# count_threshold = 2000000\n",
    "\n",
    "# start_year = 1984\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# print_count = 20\n",
    "# image_size = 1400\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '../v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_list = list(pd.read_csv(scene_skip_url)['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# red_band = 'SR_RED'\n",
    "# green_band = 'SR_GREEN'\n",
    "# blue_band = 'SR_BLUE'\n",
    "\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2_tile in wrs2_list:\n",
    "#     # if int(wrs2_tile[1:4]) not in range(10, 25):\n",
    "#     #     continue\n",
    "    \n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "#             os.remove(wrs2_stats_path)\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df.drop(columns=['system:index', '.geo'], inplace=True)\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "    \n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the ratios\n",
    "# stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS']\n",
    "#     + stats_df['CIRRUS_PIXELS']\n",
    "#     + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     # + stats_df['WATER_PIXELS']\n",
    "#     + stats_df['ACCA_PIXELS']\n",
    "#     # + stats_df['SATURATED_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# # wrs2_tiles = ['']\n",
    "\n",
    "# # new_skip_count = 0\n",
    "# # new_skip_scenes = []\n",
    "\n",
    "# # for wrs2 in reversed(wrs2_tiles):\n",
    "# # for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "#     # if wrs2_i > 20:\n",
    "#     #     break\n",
    "#     if wrs2 in ['p021r040', 'p038r041']:\n",
    "#         continue\n",
    "#     # if wrs2 in ['p033r037', 'p039r031', 'p039r032', 'p039r033', 'p042r034', 'p042r032']:\n",
    "#     #     continue\n",
    "#     # if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#     #     continue\n",
    "#     if ocean_wrs2_list and (wrs2 in ocean_wrs2_list):\n",
    "#         continue\n",
    "#     # if int(wrs2[1:4]) not in range(10, 24):\n",
    "#     #     continue\n",
    "#     # if int(wrs2[5:8]) not in range(35, 50):\n",
    "#     #     continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "#     # # Only look at Landsat 8 and 9 for this test\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SCENE_ID'].str.slice(0,4).isin(['LC08', 'LC09'])]\n",
    "\n",
    "#     # # Filter on the snow pixel count ratio\n",
    "#     # wrs2_stats_df = wrs2_stats_df[stats_df['SNOW_COUNT_RATIO'] > 0.8]\n",
    "#     # wrs2_stats_df.sort_values('SNOW_COUNT_RATIO', ascending=False, inplace=True)\n",
    "    \n",
    "#     # # Filter on the overall cloud count ratio\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     # # wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "    \n",
    "#     # # Only check summer scenes\n",
    "#     # stats_df = stats_df[stats_df['DATE'].str.slice(4,6).astype(int).isin([5, 6, 7, 8, 9])]\n",
    "    \n",
    "#     # Only check winter scenes\n",
    "#     # stats_df = stats_df[~stats_df['DATE'].str.slice(4,6).astype(int).isin(range(4, 11))]\n",
    "#     # stats_df = stats_df[~stats_df['DATE'].str.slice(4,6).astype(int).isin(range(5, 10))]\n",
    "#     # stats_df = stats_df[~stats_df['DATE'].str.slice(4,6).astype(int).isin(range(7, 10))]\n",
    "    \n",
    "#     # # Filter on the overall cloud count ratio\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     # wrs2_stats_df.sort_values('ACCA_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['UNMASKED_PIXELS'] < count_threshold]\n",
    "#     # wrs2_stats_df.sort_values('UNMASKED_PIXELS', ascending=True, inplace=True)\n",
    "#     wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "    \n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     new_skip_count = 0\n",
    "#     new_skip_scenes = []\n",
    "    \n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(10, len(wrs2_stats_df.index))).iterrows():\n",
    "\n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "#         # # Only review scenes that have the image above and below in the skip list\n",
    "#         # if above_scene_id not in scene_skip_list:\n",
    "#         #     continue\n",
    "#         # if below_scene_id not in scene_skip_list:\n",
    "#         #     continue\n",
    "\n",
    "#         above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "#         above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "#         if len(above_stats_df):\n",
    "#             above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             above_cloud_pct = None\n",
    "            \n",
    "#         below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "#         below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "#         if len(below_stats_df):\n",
    "#             below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             below_cloud_pct = None\n",
    "\n",
    "#         # # Only show scenes that have above & below both skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "        \n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[red_band]:0.2f}  {row[green_band]:0.2f}  {row[blue_band]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "#         # Show the images above and below the target wrs2\n",
    "#         above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "#         above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             above_url = (\n",
    "#                 above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             above_url = None\n",
    "            \n",
    "#         below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "#         below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             below_url = (\n",
    "#                 below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             below_url = None\n",
    "    \n",
    "#         above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "#         below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "#         if above_url and below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "#         elif above_url:\n",
    "#             print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([above_url], img_width=image_size)\n",
    "#         elif below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "#             ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "#         new_skip_count += 1\n",
    "#         new_skip_scenes.append(scene_id)\n",
    "#         if new_skip_count >= print_count:\n",
    "#             break\n",
    "\n",
    "#     if new_skip_scenes:\n",
    "#         for scene_id in new_skip_scenes:\n",
    "#             print(scene_id)\n",
    "#     if new_skip_count:\n",
    "#         wrs2_i += 1\n",
    "        \n",
    "# # if new_skip_scenes:\n",
    "# #     for scene_id in new_skip_scenes:\n",
    "# #         print(scene_id)\n",
    "\n",
    "# print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c665f-0bb6-442b-b877-b111bc3a6bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36cff9-d387-4c70-8060-14a62459e743",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes with high average reflectance\n",
    "# # refl_min = 0.4\n",
    "# # refl_min = 0.35\n",
    "# # refl_min = 0.3\n",
    "# # refl_min = 0.25\n",
    "# refl_min = 0.2\n",
    "# # refl_min = 0.19\n",
    "# # refl_min = 0.18\n",
    "# # refl_min = -0.1\n",
    "\n",
    "# refl_max = 1.5\n",
    "# # refl_max = 0.5\n",
    "\n",
    "# #image_size = 900\n",
    "# #image_size = 1024\n",
    "# image_size = 1400\n",
    "# print_count = 10\n",
    "\n",
    "# #start_year = 1984\n",
    "# #start_year = 2015\n",
    "# start_year = 2024\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# # # These are the average reflectance values after masking (for the unmasked pixels)\n",
    "# red_band = 'UNMASKED_SR_RED'\n",
    "# green_band = 'UNMASKED_SR_GREEN'\n",
    "# blue_band = 'UNMASKED_SR_BLUE'\n",
    "# # red_band = 'UNMASKED_TOA_RED'\n",
    "# # green_band = 'UNMASKED_TOA_GREEN'\n",
    "# # blue_band = 'UNMASKED_TOA_BLUE'\n",
    "\n",
    "# # # These are the average reflectance values for the full scene before masking\n",
    "# # red_band = 'SR_RED'\n",
    "# # green_band = 'SR_GREEN'\n",
    "# # blue_band = 'SR_BLUE'\n",
    "# # red_band = 'TOA_RED'\n",
    "# # green_band = 'TOA_GREEN'\n",
    "# # blue_band = 'TOA_BLUE'\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '/Users/Charles.Morton@dri.edu/Projects/scene-skip-list/v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '/Users/Charles.Morton@dri.edu/Projects/scene-skip-list/v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# scene_keep_list = [\n",
    "#     # 'LE07_039032_20000604',\n",
    "#     # 'LE07_039032_19990805',\n",
    "#     # 'LE07_039032_19990704',\n",
    "#     # 'LE07_039032_20000604',\n",
    "#     # 'LE07_037038_20110707',\n",
    "#     # 'LE07_033038_20000525',\n",
    "#     # 'LE07_040030_20190107',\n",
    "#     # 'LE07_037032_20211006',\n",
    "#     # 'LE07_041036_20001008',\n",
    "# ]\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2 in wrs2_list:\n",
    "#     # if int(wrs2[1:4]) not in range(31, 35):\n",
    "#     #     continue\n",
    "    \n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2}_{year} - Error reading CSV, skipping')\n",
    "#             os.remove(wrs2_stats_path)\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df.drop(columns=['system:index', '.geo'], inplace=True)\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # # Skip and keep scenes will both be skipped in this processing since they have already been reviewed\n",
    "# # if scene_skip_list:\n",
    "# #     stats_df = stats_df[~stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "# # if scene_keep_list:\n",
    "# #     stats_df = stats_df[~stats_df['SCENE_ID'].isin(scene_keep_list)]\n",
    "\n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the cloud count ratios\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['SNOW_COUNT_RATIO'] = (stats_df['SNOW_PIXELS'] + stats_df['SHADOW_PIXELS'] + stats_df['WATER_PIXELS'] + stats_df['DILATE_PIXELS']) / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS']\n",
    "#     + stats_df['CIRRUS_PIXELS']\n",
    "#     + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     # + stats_df['WATER_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "# print('  Done\\n')\n",
    "\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# # wrs2_tiles = ['']\n",
    "\n",
    "# new_skip_scenes = []\n",
    "# wrs2_i = 0\n",
    "\n",
    "# # for wrs2 in reversed(wrs2_tiles):\n",
    "# # for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "#     if wrs2_i > 20:\n",
    "#         break\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     # if wrs2 in ['p040r036', 'p040r036', 'p039r032', 'p038r038']:\n",
    "#     #     continue\n",
    "#     # if california_wrs2_list and (wrs2 not in california_wrs2_list):\n",
    "#     #     continue\n",
    "#     # if int(wrs2[1:4]) not in range(29, 35):\n",
    "#     #     continue\n",
    "#     if int(wrs2[5:8]) >= 30:\n",
    "#         continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "#     # # Only look at Landsat 8 and 9 for this test\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SCENE_ID'].str.slice(0,4).isin(['LC08', 'LC09'])]\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'] >= '19841201']\n",
    "\n",
    "#     # Compute the average reflectance\n",
    "#     wrs2_stats_df = wrs2_stats_df[(wrs2_stats_df[red_band] > refl_min) | (wrs2_stats_df[green_band] > refl_min) | (wrs2_stats_df[blue_band] > refl_min)]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[(wrs2_stats_df[red_band] > refl_min) & (wrs2_stats_df[green_band] > refl_min) & (wrs2_stats_df[blue_band] > refl_min)]\n",
    "#     # wrs2_stats_df.sort_values(red_band, ascending=False, inplace=True)\n",
    "#     wrs2_stats_df['REFL_SORT'] = (wrs2_stats_df[red_band] + wrs2_stats_df[green_band] + wrs2_stats_df[blue_band]) / 3\n",
    "    \n",
    "#     # Filter on the overall cloud count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['REFL_SORT'] < (refl_max)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['REFL_SORT'] >= (refl_min)]\n",
    "#     wrs2_stats_df.sort_values('REFL_SORT', ascending=False, inplace=True)\n",
    "#     # wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "    \n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         # print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     new_skip_count = 0\n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(10, len(wrs2_stats_df.index))).iterrows():\n",
    "        \n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "        \n",
    "#         above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "#         above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "#         if len(above_stats_df):\n",
    "#             above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             above_cloud_pct = None\n",
    "            \n",
    "#         below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "#         below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "#         if len(below_stats_df):\n",
    "#             below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             below_cloud_pct = None\n",
    "\n",
    "#         # # Only show scenes that have above & below both skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "        \n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[red_band]:0.2f}  {row[green_band]:0.2f}  {row[blue_band]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "    \n",
    "#         # Show the images above and below the target wrs2\n",
    "#         above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "#         above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             above_url = (\n",
    "#                 above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             above_url = None\n",
    "            \n",
    "#         below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "#         below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             below_url = (\n",
    "#                 below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             below_url = None\n",
    "    \n",
    "#         above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "#         below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "#         if above_url and below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "#         elif above_url:\n",
    "#             print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([above_url], img_width=image_size)\n",
    "#         elif below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "#             ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "#         new_skip_count += 1\n",
    "#         if new_skip_count >= print_count:\n",
    "#             break\n",
    "\n",
    "#     # if new_skip_scenes:\n",
    "#     #     for scene_id in new_skip_scenes:\n",
    "#     #         print(scene_id)\n",
    "#     if new_skip_count:\n",
    "#         wrs2_i += 1\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee6d5d-d9ca-48d1-88f7-7a75f521310a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817eedfe-c450-44dc-81be-64503b412781",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes with low pixel count ratios (few unmasked pixels)\n",
    "# count_threshold_pct_min = 70\n",
    "# count_threshold_pct_max = 100\n",
    "# count_threshold = 1000000\n",
    "\n",
    "# start_year = 1984\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# print_count = 10\n",
    "# # image_size = 700\n",
    "# # image_size = 900\n",
    "# # image_size = 1024\n",
    "# image_size = 1400\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '../v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2_tile in wrs2_list:\n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "    \n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the ratios\n",
    "# # stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['SHADOW_COUNT_RATIO'] = stats_df['SHADOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     + stats_df['WATER_PIXELS']\n",
    "#     + stats_df['ACCA_PIXELS']\n",
    "#     # + stats_df['SATURATED_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# # wrs2_tiles = ['']\n",
    "\n",
    "# # new_skip_scenes = []\n",
    "# # new_skip_count = 0\n",
    "\n",
    "# wrs2_i = 0\n",
    "\n",
    "# # for wrs2 in reversed(wrs2_tiles):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# # for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "#     # if wrs2_i > 20:\n",
    "#     #     break\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     # if wrs2 in ['p046r033', 'p047r031']:\n",
    "#     # if wrs2 in ['p011r030', 'p011r031', 'p012r032', 'p015r040', 'p021r040', 'p024r040', 'p041r037']:\n",
    "#     #     continue\n",
    "#     if ocean_wrs2_list and (wrs2 in ocean_wrs2_list):\n",
    "#         continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "    \n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "\n",
    "#     # # Only look at Landsat 8 and 9 for this test\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SCENE_ID'].str.slice(0,4).isin(['LC08', 'LC09'])]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'] >= '19841201']\n",
    "\n",
    "#     # # Filter on the CLOUD_COVER_LAND property\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 60]\n",
    "#     # wrs2_stats_df.sort_values('CLOUD_COVER_LAND', ascending=False, inplace=True)\n",
    "\n",
    "#     # Filter on the overall cloud count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     # Filter on the masked pixel count\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['UNMASKED_PIXELS'] < count_threshold]\n",
    "#     wrs2_stats_df.sort_values('UNMASKED_PIXELS', ascending=True, inplace=True)\n",
    "\n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     new_skip_scenes = []\n",
    "#     new_skip_count = 0\n",
    "    \n",
    "#     # for i, row in wrs2_stats_df.sample(n=min(10, len(wrs2_stats_df.index))).iterrows():\n",
    "#     for i, row in wrs2_stats_df.iterrows():\n",
    "\n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "#         above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "#         above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "#         if len(above_stats_df):\n",
    "#             above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             above_cloud_pct = None\n",
    "            \n",
    "#         below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "#         below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "#         if len(below_stats_df):\n",
    "#             below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             below_cloud_pct = None\n",
    "\n",
    "#         # # Only show scenes that have above & below both skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "    \n",
    "#         # # Only show scenes that have either the above & below scene skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) and \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue  \n",
    "\n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "    \n",
    "#         # Show the images above and below the target wrs2\n",
    "#         above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "#         above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             above_url = (\n",
    "#                 above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             above_url = None\n",
    "            \n",
    "#         below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "#         below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             below_url = (\n",
    "#                 below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             below_url = None\n",
    "    \n",
    "#         above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "#         below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "#         if above_url and below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "#         elif above_url:\n",
    "#             print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([above_url], img_width=image_size)\n",
    "#         elif below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "#             ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "#         new_skip_scenes.append(scene_id)\n",
    "#         new_skip_count += 1\n",
    "#         if new_skip_count >= print_count:\n",
    "#             break\n",
    "\n",
    "#     if new_skip_scenes:\n",
    "#         wrs2_i += 1\n",
    "#         for scene_id in new_skip_scenes:\n",
    "#             print(scene_id)\n",
    "        \n",
    "# # if new_skip_scenes:\n",
    "# #     for scene_id in new_skip_scenes:\n",
    "# #         print(scene_id)\n",
    "\n",
    "# print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e62f3-3bc5-47ed-8480-9ae46884cf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbc4de-b3d6-43da-8030-a8464bc02dc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes with high numbers of unmasked ACCA cloudscore pixels\n",
    "# count_threshold_pct_min = 0.1\n",
    "# count_threshold_pct_max = 100\n",
    "\n",
    "# #count_threshold_min = 1000000\n",
    "# #count_threshold_max = 2000000\n",
    "# count_threshold_min = 10000000\n",
    "# count_threshold_max = 100000000\n",
    "\n",
    "# #image_size = 900\n",
    "# #image_size = 1024\n",
    "# image_size = 1400\n",
    "# print_count = 10\n",
    "\n",
    "# start_year = 1985\n",
    "# #start_year = 2003\n",
    "# #start_year = 2015\n",
    "# start_year = 2024\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# red_band = 'SR_RED'\n",
    "# green_band = 'SR_GREEN'\n",
    "# blue_band = 'SR_BLUE'\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '/Users/Charles.Morton@dri.edu/Projects/scene-skip-list/v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '/Users/Charles.Morton@dri.edu/Projects/scene-skip-list/v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "# # scene_keep_list = []\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2 in wrs2_list:\n",
    "#     # if int(wrs2[1:4]) not in range(10, 25):\n",
    "#     #     continue\n",
    "    \n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2}_{year} - Error reading CSV, skipping')\n",
    "#             os.remove(wrs2_stats_path)\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df.drop(columns=['system:index', '.geo'], inplace=True)\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the cloud count ratios\n",
    "# stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SATURATED_COUNT_RATIO'] = stats_df['SATURATED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['SNOW_COUNT_RATIO'] = (stats_df['SNOW_PIXELS'] + stats_df['SHADOW_PIXELS'] + stats_df['WATER_PIXELS'] + stats_df['DILATE_PIXELS']) / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS']\n",
    "#     + stats_df['CIRRUS_PIXELS']\n",
    "#     + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     # + stats_df['WATER_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "# print('  Done\\n')\n",
    "\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "\n",
    "\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     # if california_wrs2_list and (wrs2 not in california_wrs2_list):\n",
    "#     #     continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "\n",
    "#     # Filter on the overall cloud count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_PIXELS'] > count_threshold_min]\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_PIXELS'] < count_threshold_max]\n",
    "#     #wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     #wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     #wrs2_stats_df.sort_values('ACCA_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     # # Filter on the overall cloud count ratio\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SATURATED_PIXELS'] > count_threshold]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SATURATED_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SATURATED_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     # wrs2_stats_df.sort_values('SATURATED_COUNT_RATIO', ascending=False, inplace=True)\n",
    "    \n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         # print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "        \n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "#         # # Only review scenes that have the image above and below in the skip list\n",
    "#         # if above_scene_id not in scene_skip_list:\n",
    "#         #     continue\n",
    "#         # if below_scene_id not in scene_skip_list:\n",
    "#         #     continue\n",
    "        \n",
    "#         above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "#         above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "#         if len(above_stats_df):\n",
    "#             above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             above_cloud_pct = None\n",
    "            \n",
    "#         below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "#         below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "#         if len(below_stats_df):\n",
    "#             below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             below_cloud_pct = None\n",
    "\n",
    "#         # # Only show scenes that have above & below both skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "        \n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[red_band]:0.2f}  {row[green_band]:0.2f}  {row[blue_band]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "    \n",
    "#         # Show the images above and below the target wrs2\n",
    "#         above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "#         above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             above_url = (\n",
    "#                 above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             above_url = None\n",
    "            \n",
    "#         below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "#         below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             below_url = (\n",
    "#                 below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             below_url = None\n",
    "    \n",
    "#         above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "#         below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "#         if above_url and below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "#         elif above_url:\n",
    "#             print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([above_url], img_width=image_size)\n",
    "#         elif below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "#             ipyplot.plot_images([below_url], img_width=image_size)\n",
    "\n",
    "# print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce0dbe-c768-4fef-b885-24a03f49ed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1254575-881b-40ae-bcc9-167021f9d88d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes with lots of snowy pixels\n",
    "# count_threshold_pct_min = 70\n",
    "# count_threshold_pct_max = 101\n",
    "\n",
    "# # snow_threshold_pct_min = 80\n",
    "# # snow_threshold_pct_max = 101\n",
    "# snow_threshold_pct_min = 70\n",
    "# snow_threshold_pct_max = 101\n",
    "\n",
    "# start_year = 1984\n",
    "# #start_year = 2003\n",
    "# #start_year = 2015\n",
    "# #start_year = 2024\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# print_count = 10\n",
    "# # image_size = 700\n",
    "# # image_size = 900\n",
    "# # image_size = 1024\n",
    "# image_size = 1400\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '../v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2_tile in wrs2_list:\n",
    "#     # if int(wrs2_tile[1:4]) not in range(10, 25):\n",
    "#     #     continue\n",
    "        \n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "# # # Filter by date\n",
    "# # if start_date:\n",
    "# #     stats_df = stats_df[stats_df['DATE'].str >= start_date.replace('-', '')]\n",
    "# # if start_date:\n",
    "# #     stats_df = stats_df[stats_df['DATE'].str < end_date.replace('-', '')]\n",
    "\n",
    "# # Compute the ratios\n",
    "# # stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['SHADOW_COUNT_RATIO'] = stats_df['SHADOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     + stats_df['WATER_PIXELS']\n",
    "#     + stats_df['ACCA_PIXELS']\n",
    "#     # + stats_df['SATURATED_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# # wrs2_tiles = ['']\n",
    "\n",
    "# new_skip_scenes = []\n",
    "# new_skip_count = 0\n",
    "\n",
    "# wrs2_i = 0\n",
    "\n",
    "# # for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "#     if wrs2_i > 10:\n",
    "#         break\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     # if int(wrs2[5:8]) >= 30:\n",
    "#     #     continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "\n",
    "#     # # Only check winter scenes\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([11, 12, 1, 2])]\n",
    "\n",
    "#     # Filter on the snow pixel count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SNOW_COUNT_RATIO'] < (snow_threshold_pct_max / 100)].copy()\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SNOW_COUNT_RATIO'] >= (snow_threshold_pct_min / 100)].copy()\n",
    "#     # wrs2_stats_df.sort_values('SNOW_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     # Filter on the snow pixel count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)].copy()\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)].copy()\n",
    "\n",
    "#     # # Filter on the CLOUD_COVER_LAND property\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 60]\n",
    "\n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     new_skip_scenes = []\n",
    "#     new_skip_count = 0\n",
    "    \n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "\n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "#         above_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_above)\n",
    "#         above_stats_df = stats_df.loc[stats_df['SCENE_ID'] == above_scene_id]\n",
    "#         if len(above_stats_df):\n",
    "#             above_cloud_pct = float(above_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             above_cloud_pct = None\n",
    "            \n",
    "#         below_scene_id = scene_id.upper().replace(wrs2_tgt, wrs2_below)\n",
    "#         below_stats_df = stats_df.loc[stats_df['SCENE_ID'] == below_scene_id]\n",
    "#         if len(below_stats_df):\n",
    "#             below_cloud_pct = float(below_stats_df.iloc[0]['CLOUD_COVER_LAND'])\n",
    "#         else:\n",
    "#             below_cloud_pct = None\n",
    "\n",
    "#         # # Only show scenes that have above & below both skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) or \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "\n",
    "#         # # Only show scenes that have either above & below skipped or None\n",
    "#         # if (((above_scene_id not in scene_skip_list) and (above_cloud_pct is not None)) and \n",
    "#         #     ((below_scene_id not in scene_skip_list) and (below_cloud_pct is not None))):\n",
    "#         #     continue   \n",
    "            \n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "#         # Show the images above and below the target wrs2\n",
    "#         above_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{above_scene_id}')\n",
    "#         above_region = above_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         above_sr_img = above_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             above_url = (\n",
    "#                 above_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(above_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': above_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             above_url = None\n",
    "            \n",
    "#         below_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{below_scene_id}')\n",
    "#         below_region = below_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         below_sr_img = below_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "#         try:\n",
    "#             below_url = (\n",
    "#                 below_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#                 .blend(fmask(below_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#                 .getThumbURL({'region': below_region, 'dimensions': image_size})\n",
    "#             )\n",
    "#         except:\n",
    "#             below_url = None\n",
    "    \n",
    "#         above_skipped = f' (skipped)' if above_scene_id in scene_skip_list else ''   \n",
    "#         below_skipped = f' (skipped)' if below_scene_id in scene_skip_list else ''\n",
    "        \n",
    "#         if above_url and below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}  {above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([below_url, above_url], img_width=image_size)\n",
    "#         elif above_url:\n",
    "#             print(f'{above_scene_id} ({above_cloud_pct}){above_skipped}')\n",
    "#             ipyplot.plot_images([above_url], img_width=image_size)\n",
    "#         elif below_url:\n",
    "#             print(f'{below_scene_id} ({below_cloud_pct}){below_skipped}')\n",
    "#             ipyplot.plot_images([below_url], img_width=image_size)\n",
    "    \n",
    "#         new_skip_scenes.append(scene_id)\n",
    "#         new_skip_count += 1\n",
    "#         if new_skip_count >= print_count:\n",
    "#             break\n",
    "\n",
    "#     if new_skip_scenes:\n",
    "#         for scene_id in new_skip_scenes:\n",
    "#             print(scene_id)\n",
    "#     if new_skip_count:\n",
    "#         wrs2_i += 1\n",
    "\n",
    "# # print('\\nNew Skip Scenes')\n",
    "# # if new_skip_scenes:\n",
    "# #     for scene_id in new_skip_scenes:\n",
    "# #         print(scene_id)\n",
    "\n",
    "# print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e7ddb-594c-4e14-b484-b80821ebaefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7734b-ae40-4fb7-ab9a-6cb938d7a909",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Identify scenes with lots of ACCA pixels that aren't flagged as missing\n",
    "# # acca_threshold_pct_min = 10\n",
    "# # acca_threshold_pct_max = 101\n",
    "\n",
    "# #acca_threshold_count_min = 10000000\n",
    "# #acca_threshold_count_min = 5000000\n",
    "# #acca_threshold_count_min = 2000000\n",
    "# #acca_threshold_count_min = 1500000\n",
    "# acca_threshold_count_min = 1000000\n",
    "\n",
    "# cloud_threshold_pct_min = 0\n",
    "# cloud_threshold_pct_max = 101\n",
    "\n",
    "# start_year = 1984\n",
    "# #start_year = 2015\n",
    "# #start_year = 2024\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# print_count = 10\n",
    "# image_size = 1400\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '../v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2_tile in wrs2_list:\n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the ratios\n",
    "# stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     + stats_df['WATER_PIXELS']\n",
    "#     + stats_df['ACCA_PIXELS']\n",
    "#     # + stats_df['SATURATED_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "# # wrs2_tiles = ['']\n",
    "\n",
    "# new_skip_scenes = []\n",
    "# new_skip_count = 0\n",
    "\n",
    "# wrs2_i = 0\n",
    "\n",
    "# # for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "#     if wrs2_i > 20:\n",
    "#         break\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     if wrs2 in ['p039r032', 'p039r031']:\n",
    "#         continue\n",
    "\n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "    \n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "    \n",
    "#     # Check for scenes in the skip list that should be flagged as missing\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "    \n",
    "#     # # Check for scenes not in the skip list that should be flagged as missing (and added to the skip list)\n",
    "#     # wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "\n",
    "#     # Skip all of the cloud score scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "\n",
    "#     # Only check scenes that aren't flagged as missing\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_df[scene_skip_df['REASON'].str.contains('Missing')]['SCENE_ID'].values)].copy()\n",
    "\n",
    "#     # Filter on the ACCA pixel count\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_PIXELS'] > (acca_threshold_count_min)].copy()\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['ACCA_PIXELS'] < (acca_threshold_count_max)].copy()\n",
    "\n",
    "#     # Filter on the acca pixel count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (cloud_threshold_pct_max / 100)].copy()\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (cloud_threshold_pct_min / 100)].copy()\n",
    "#     #wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     # # Filter on the CLOUD_COVER_LAND property\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 60]\n",
    "\n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     new_skip_scenes = []\n",
    "#     new_skip_count = 0\n",
    "    \n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "            \n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "    \n",
    "#         new_skip_scenes.append(scene_id)\n",
    "#         new_skip_count += 1\n",
    "#         if new_skip_count >= print_count:\n",
    "#             break\n",
    "\n",
    "#     if new_skip_scenes:\n",
    "#         for scene_id in new_skip_scenes:\n",
    "#             print(scene_id)\n",
    "#     if new_skip_count:\n",
    "#         wrs2_i += 1\n",
    "\n",
    "# print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf74d41-256e-472d-ac24-d4aa683f2fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b15b18-0818-4a63-858f-2aebd663fc43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Print scenes that are totally masked\n",
    "# count_threshold_pct_min = 90\n",
    "# #count_threshold_pct_min = 95\n",
    "# #count_threshold_pct_min = 99\n",
    "# count_threshold_pct_max = 101\n",
    "\n",
    "# #start_year = 1984\n",
    "# #start_year = 2003\n",
    "# #start_year = 2015\n",
    "# #end_year = 2024\n",
    "# start_year = 2024\n",
    "# end_year = 2025\n",
    "# years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# print_count = 20\n",
    "# #image_size = 700\n",
    "# #image_size = 1024\n",
    "# image_size = 1400\n",
    "\n",
    "# # Read in the scene skip list\n",
    "# scene_skip_url = '../v2p1.csv'\n",
    "# # scene_skip_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1.csv'\n",
    "# scene_skip_df = pd.read_csv(scene_skip_url)\n",
    "# scene_skip_list = list(scene_skip_df['SCENE_ID'].values)\n",
    "# print(f'Skip list images: {len(scene_skip_list)}')\n",
    "\n",
    "# scene_cloudscore_url = '../v2p1_cloudscore.csv'\n",
    "# # scene_cloudscore_url = 'https://raw.githubusercontent.com/cgmorton/scene-skip-list/main/v2p1_cloudscore.csv'\n",
    "# scene_cloudscore_list = list(pd.read_csv(scene_cloudscore_url)['SCENE_ID'].values)\n",
    "# print(f'Skip cloudscore images: {len(scene_cloudscore_list)}')\n",
    "\n",
    "\n",
    "# print('Reading image stats CSV files')\n",
    "# stats_df_list = []\n",
    "# for wrs2_tile in wrs2_list:\n",
    "#     # if int(wrs2_tile[1:4]) not in range(10, 25):\n",
    "#     #     continue\n",
    "        \n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         wrs2_stats_path = os.path.join(stats_ws, f'{year}', f'{wrs2_tile}_{year}.csv')\n",
    "#         if not os.path.isfile(wrs2_stats_path):\n",
    "#             # print(f'  {wrs2_tile}_{year} - Missing stats CSV, skipping')\n",
    "#             continue\n",
    "#         try:\n",
    "#             wrs2_stats_df = pd.read_csv(wrs2_stats_path, index_col=False)\n",
    "#         except Exception as e:\n",
    "#             print(f'  {wrs2_tile}_{year} - Error reading CSV, skipping')\n",
    "#             continue\n",
    "#         if wrs2_stats_df.empty:\n",
    "#             continue\n",
    "#         wrs2_stats_df['DATE'] = wrs2_stats_df['SCENE_ID'].str.slice(12, 20)\n",
    "#         wrs2_stats_df['WRS2'] = 'p' + wrs2_stats_df['SCENE_ID'].str.slice(5, 8) + 'r' + wrs2_stats_df['SCENE_ID'].str.slice(8, 11)\n",
    "#         stats_df_list.append(wrs2_stats_df)\n",
    "\n",
    "# stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "# # skip_stats_df = stats_df[stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "# # wrs2_tiles = list(skip_stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "\n",
    "# # Add the high CLOUD_COVER_LAND scenes to the skip list but don't remove from the dataframe\n",
    "# scene_skip_list.extend(stats_df[stats_df['CLOUD_COVER_LAND'] >= 71]['SCENE_ID'].values)\n",
    "\n",
    "# # Skip the Landsat 7 scenes in 2023\n",
    "# l7_2022_mask = (\n",
    "#     (stats_df['DATE'].str.slice(0,4) >= '2022') &\n",
    "#     (stats_df['SCENE_ID'].str.slice(0,4) == 'LE07')\n",
    "# )\n",
    "# stats_df = stats_df[~l7_2022_mask]\n",
    "\n",
    "# # Compute the ratios\n",
    "# # stats_df['ACCA_COUNT_RATIO'] = stats_df['ACCA_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['SNOW_COUNT_RATIO'] = stats_df['SNOW_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['WATER_COUNT_RATIO'] = stats_df['WATER_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# stats_df['MASKED_PIXELS'] = (\n",
    "#     stats_df['CLOUD_PIXELS'] + stats_df['CIRRUS_PIXELS'] + stats_df['DILATE_PIXELS']\n",
    "#     + stats_df['SHADOW_PIXELS']\n",
    "#     + stats_df['SNOW_PIXELS']\n",
    "#     + stats_df['WATER_PIXELS']\n",
    "#     + stats_df['ACCA_PIXELS']\n",
    "#     # + stats_df['SATURATED_PIXELS']\n",
    "# )\n",
    "# stats_df['CLOUD_COUNT_RATIO'] = stats_df['MASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "# # stats_df['CLOUD_COUNT_RATIO'] = stats_df['UNMASKED_PIXELS'] / stats_df['TOTAL_PIXELS']\n",
    "\n",
    "# print(f'  {len(stats_df.count(axis=1))}')\n",
    "\n",
    "# # Work through the tiles based on which ones already have the most skipped scenes\n",
    "# wrs2_tiles = list(stats_df.groupby(['WRS2'])['SCENE_ID'].count().sort_values(ascending=False).index)\n",
    "\n",
    "# new_skip_scenes = []\n",
    "# new_skip_count = 0\n",
    "# wrs2_i = 0\n",
    "\n",
    "# # for wrs2 in reversed(wrs2_tiles):\n",
    "# # for wrs2 in reversed(sorted(wrs2_tiles)):\n",
    "# # for wrs2 in sorted(wrs2_tiles):\n",
    "# for wrs2 in random.sample(wrs2_tiles, len(wrs2_tiles)):\n",
    "#     if wrs2_i >= 10:\n",
    "#         break\n",
    "#     if wrs2_skip_list and (wrs2 in wrs2_skip_list):\n",
    "#         continue\n",
    "#     # if california_wrs2_list and (wrs2 not in california_wrs2_list) and wrs2 not in ['p042r033']:\n",
    "#     #     continue\n",
    "#     # if int(wrs2[5:8]) >= 30:\n",
    "#     #     continue\n",
    "    \n",
    "#     wrs2_path = int(wrs2[1:4])\n",
    "#     wrs2_row = int(wrs2[5:8])\n",
    "#     wrs2_tgt = f'{wrs2_path:03d}{wrs2_row:03d}'\n",
    "#     wrs2_above = f'{wrs2_path:03d}{wrs2_row-1:03d}'\n",
    "#     wrs2_below = f'{wrs2_path:03d}{wrs2_row+1:03d}'    \n",
    "\n",
    "#     wrs2_stats_df = stats_df[stats_df['WRS2'] == wrs2].copy()\n",
    "#     # Applying skip list here so that main stats DF has all scenes\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_skip_list)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[~wrs2_stats_df['SCENE_ID'].isin(scene_cloudscore_list)]\n",
    "    \n",
    "#     # # Only check winter scenes\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([11, 12, 1, 2, 3])]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([10, 11, 12, 1, 2, 3, 4])]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['DATE'].str.slice(4,6).astype(int).isin([6, 7, 8])]\n",
    "\n",
    "#     # Filter on the overall cloud count ratio\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] < (count_threshold_pct_max / 100)]\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COUNT_RATIO'] >= (count_threshold_pct_min / 100)]\n",
    "#     wrs2_stats_df.sort_values('CLOUD_COUNT_RATIO', ascending=False, inplace=True)\n",
    "\n",
    "#     # # Filter on the CLOUD_COVER_LAND property\n",
    "#     wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] < 71]\n",
    "#     # wrs2_stats_df = wrs2_stats_df[wrs2_stats_df['CLOUD_COVER_LAND'] >= 60]\n",
    "#     # wrs2_stats_df.sort_values('CLOUD_COVER_LAND', ascending=False, inplace=True)\n",
    "\n",
    "#     if len(wrs2_stats_df.count(axis=1)) == 0:\n",
    "#         continue\n",
    "#     print(f'{wrs2} - {len(wrs2_stats_df.count(axis=1))}')\n",
    "\n",
    "#     wrs2_skip_scenes = []\n",
    "#     wrs2_skip_count = 0\n",
    "    \n",
    "#     # for i, row in wrs2_stats_df.iterrows():\n",
    "#     for i, row in wrs2_stats_df.sample(n=min(print_count, len(wrs2_stats_df.index))).iterrows():\n",
    "\n",
    "#         scene_id = row[\"SCENE_ID\"].upper()\n",
    "\n",
    "#         landsat_type = scene_id.split('_')[0].upper()\n",
    "#         landsat_img = ee.Image(f'LANDSAT/{landsat_type}/C02/T1_L2/{scene_id}')\n",
    "#         landsat_region = landsat_img.geometry().bounds(1, 'EPSG:4326')\n",
    "#         landsat_sr_img = landsat_img.select(rgb_bands[landsat_type]).multiply([0.0000275]).add([-0.2])\n",
    "\n",
    "#         # Landsat true color image\n",
    "#         landsat_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25)\n",
    "#             .getThumbURL({'min': 0.0, 'max': 0.30, 'gamma': 1.25, 'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         # Landsat true color with Fmask\n",
    "#         fmask_url = (\n",
    "#             landsat_sr_img.where(land_mask.unmask().eq(0), 0.25).visualize(min=0, max=0.3, gamma=1.25)\n",
    "#             .blend(fmask(landsat_img).where(land_mask.unmask().eq(0), fmask_max).visualize(bands='fmask', min=0, max=fmask_max, palette=fmask_palette))\n",
    "#             .getThumbURL({'region': landsat_region, 'dimensions': image_size})\n",
    "#         )\n",
    "    \n",
    "#         print('#'*80)\n",
    "#         print(\n",
    "#             f'  {scene_id}  {row[\"TOTAL_PIXELS\"]:>10d}  {row[\"UNMASKED_PIXELS\"]:>10d}'\n",
    "#             f'  ({row[\"CLOUD_COUNT_RATIO\"]:>0.2f}) ({row[\"SNOW_COUNT_RATIO\"]:>0.2f}) {row[\"CLOUD_COVER_LAND\"]}'\n",
    "#             f'  {row[\"SR_RED\"]:0.2f}  {row[\"SR_GREEN\"]:0.2f}  {row[\"SR_BLUE\"]:0.2f}'\n",
    "#         )\n",
    "#         ipyplot.plot_images([landsat_url, fmask_url], img_width=image_size)\n",
    "        \n",
    "#         new_skip_scenes.append(scene_id)\n",
    "#         wrs2_skip_scenes.append(scene_id)\n",
    "\n",
    "#     if wrs2_skip_scenes:\n",
    "#         wrs2_i += 1\n",
    "#         for scene_id in wrs2_skip_scenes:\n",
    "#             print(scene_id)\n",
    "\n",
    "# if new_skip_scenes:\n",
    "#     print('')\n",
    "#     for scene_id in new_skip_scenes:\n",
    "#         print(scene_id)\n",
    "\n",
    "# print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaef52-3be3-462e-9eb3-1fdc0c10e71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
